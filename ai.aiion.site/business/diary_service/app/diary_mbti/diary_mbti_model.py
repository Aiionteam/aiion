"""
Diary MBTI Model
ì¼ê¸° MBTI ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ í´ë˜ìŠ¤
"""

import pandas as pd
import numpy as np
from icecream import ic
from typing import Optional


class DiaryMbtiModel:
    """ì¼ê¸° MBTI ë¶„ë¥˜ ML ëª¨ë¸ í´ë˜ìŠ¤ (ë ˆê±°ì‹œ ì§€ì›)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.models = {}  # MBTI ì°¨ì›ë³„ ëª¨ë¸ {'E_I': model, 'S_N': model, ...}
        self.vectorizer = None
        self.word2vec_model = None
        ic("DiaryMbtiModel ì´ˆê¸°í™”")
    
    def __repr__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"DiaryMbtiModel(models={len(self.models)}ê°œ, vectorizer={self.vectorizer is not None})"

# PyTorch ë° Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer,
        AutoModel,
        AutoConfig,
    )
    TORCH_AVAILABLE = True
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE = None
    ic("ê²½ê³ : torch ë˜ëŠ” transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class BERTMbtiClassifier(nn.Module):
    """BERT ê¸°ë°˜ MBTI ì°¨ì›ë³„ 3-class ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ (0=í‰ê°€ë¶ˆê°€, 1, 2)"""
    
    def __init__(
        self,
        model_name: str = "koelectro_v3_base",  # ë¡œì»¬ KoELECTRA v3 base ëª¨ë¸ (ê¸°ë³¸ê°’)
        num_labels: int = 3,  # MBTI 3-class (0=í‰ê°€ë¶ˆê°€, 1, 2)
        dropout_rate: float = 0.3,
        hidden_size: Optional[int] = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸: koelectro_v3_base)
            num_labels: í´ë˜ìŠ¤ ìˆ˜ (MBTIëŠ” 3: 0=í‰ê°€ë¶ˆê°€, 1, 2)
            dropout_rate: Dropout ë¹„ìœ¨
            hidden_size: ì¤‘ê°„ hidden layer í¬ê¸° (Noneì´ë©´ ì§ì ‘ ë¶„ë¥˜)
        """
        super().__init__()
        if not TORCH_AVAILABLE:
            raise ImportError("torchì™€ transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.num_labels = num_labels
        
        # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œì¸ì§€ í™•ì¸
        from pathlib import Path
        model_path_str = str(model_name)
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ìš°ì„ )
        if not Path(model_path_str).is_absolute():
            # 1. Docker í™˜ê²½: /app/koelectro_v3_base (ìš°ì„ )
            docker_path = Path("/app/koelectro_v3_base")
            if docker_path.exists() and docker_path.is_dir() and (docker_path / "config.json").exists():
                model_path_str = str(docker_path)
                ic(f"âœ… Docker ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ì‚¬ìš©: {model_path_str}")
            # 2. ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ: ai.aiion.site/models/koelectra
            elif model_name == "koelectro_v3_base":
                # business/diary_service/appì´ ë£¨íŠ¸ì´ë¯€ë¡œ ìƒìœ„ë¡œ ì˜¬ë¼ê°€ì„œ ì°¾ê¸°
                current_dir = Path(__file__).parent  # diary_mbti
                app_dir = current_dir.parent  # app
                service_dir = app_dir.parent  # diary_service
                business_dir = service_dir.parent  # business
                ai_dir = business_dir.parent  # ai.aiion.site
                common_model_path = ai_dir / "models" / "koelectra"
                if common_model_path.exists() and common_model_path.is_dir() and (common_model_path / "config.json").exists():
                    model_path_str = str(common_model_path)
                    ic(f"âœ… ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ì‚¬ìš©: {model_path_str}")
                else:
                    # 3. ê¸°ì¡´ ìœ„ì¹˜ (í•˜ìœ„ í˜¸í™˜ì„±)
                    potential_path = app_dir / model_path_str
                    if potential_path.exists() and potential_path.is_dir():
                        model_path_str = str(potential_path)
                    else:
                        potential_path = ai_dir / model_path_str
                        if potential_path.exists() and potential_path.is_dir():
                            model_path_str = str(potential_path)
        
        model_path = Path(model_path_str)
        is_local_model = model_path.exists() and model_path.is_dir() and (model_path / "config.json").exists()
        
        if is_local_model:
            # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
            ic(f"âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ: {model_path}")
            self.config = AutoConfig.from_pretrained(str(model_path))
            self.bert = AutoModel.from_pretrained(str(model_path))
        else:
            # HuggingFace ëª¨ë¸ ë¡œë“œ
            ic(f"ğŸŒ HuggingFace ëª¨ë¸ ë¡œë“œ: {model_name}")
            self.config = AutoConfig.from_pretrained(model_name)
            self.bert = AutoModel.from_pretrained(model_name)
        
        self.dropout = nn.Dropout(dropout_rate)
        
        # 3-class ë¶„ë¥˜ í—¤ë“œ (MBTI: 0=í‰ê°€ë¶ˆê°€, 1, 2)
        if hidden_size:
            # 2-layer ë¶„ë¥˜ê¸°
            self.classifier = nn.Sequential(
                nn.Linear(self.config.hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout_rate),
                nn.Linear(hidden_size, num_labels)  # 3-class ë¶„ë¥˜
            )
        else:
            # 1-layer ë¶„ë¥˜ê¸°
            self.classifier = nn.Linear(self.config.hidden_size, num_labels)  # 3-class ë¶„ë¥˜
        
        self.model_name = model_name
        ic(f"BERTMbtiClassifier ì´ˆê¸°í™” ì™„ë£Œ: {model_name} ({num_labels}-class ë¶„ë¥˜)")
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        token_type_ids: Optional[torch.Tensor] = None
    ):
        """
        ìˆœì „íŒŒ
        
        Args:
            input_ids: í† í° ID
            attention_mask: Attention mask
            token_type_ids: Token type IDs (ì„ íƒ)
        
        Returns:
            logits: ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ë¡œì§“ (batch_size, num_labels)
        """
        # BERT ì¸ì½”ë”©
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        # [CLS] í† í°ì˜ hidden state ì¶”ì¶œ
        pooled_output = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)
        
        # Dropout ë° ë¶„ë¥˜
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        return logits
    
    def freeze_bert_layers(self, num_layers_to_freeze: int = 8):
        """
        BERT í•˜ìœ„ ë ˆì´ì–´ë¥¼ ë™ê²°í•˜ì—¬ í•™ìŠµ ì†ë„ í–¥ìƒ
        
        Args:
            num_layers_to_freeze: ë™ê²°í•  ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸: 8)
        """
        # Embedding layer ë™ê²°
        for param in self.bert.embeddings.parameters():
            param.requires_grad = False
        
        # ì§€ì •ëœ ìˆ˜ë§Œí¼ encoder layer ë™ê²°
        for i in range(num_layers_to_freeze):
            if i < len(self.bert.encoder.layer):
                for param in self.bert.encoder.layer[i].parameters():
                    param.requires_grad = False
        
        ic(f"BERT í•˜ìœ„ {num_layers_to_freeze}ê°œ ë ˆì´ì–´ ë™ê²° ì™„ë£Œ")
    
    def unfreeze_all(self):
        """ëª¨ë“  ë ˆì´ì–´ ë™ê²° í•´ì œ"""
        for param in self.parameters():
            param.requires_grad = True
        ic("ëª¨ë“  ë ˆì´ì–´ ë™ê²° í•´ì œ ì™„ë£Œ")


class DiaryMbtiDLModel:
    """ì¼ê¸° MBTI ë¶„ë¥˜ DL ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ (4ê°œ ì°¨ì›ë³„ ëª¨ë¸)"""
    
    def __init__(
        self,
        model_name: str = "koelectro_v3_base",  # ë¡œì»¬ KoELECTRA v3 base ëª¨ë¸ (ê¸°ë³¸ê°’)
        max_length: int = 512,
        device: Optional[torch.device] = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸: koelectro_v3_base)
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ê°ì§€)
        """
        if not TORCH_AVAILABLE:
            raise ImportError("torchì™€ transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model_name = model_name
        self.max_length = max_length
        # deviceê°€ Noneì´ë©´ ëŸ°íƒ€ì„ì— ë‹¤ì‹œ í™•ì¸
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ (ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì§€ì›)
        from pathlib import Path
        model_path_str = str(self.model_name)
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ìš°ì„ )
        if not Path(model_path_str).is_absolute():
            # 1. Docker í™˜ê²½: /app/koelectro_v3_base (ìš°ì„ )
            docker_path = Path("/app/koelectro_v3_base")
            if docker_path.exists() and docker_path.is_dir() and (docker_path / "config.json").exists():
                model_path_str = str(docker_path)
                ic(f"âœ… Docker ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ì‚¬ìš©: {model_path_str}")
            # 2. ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ: ai.aiion.site/models/koelectra
            elif model_name == "koelectro_v3_base":
                # business/diary_service/appì´ ë£¨íŠ¸ì´ë¯€ë¡œ ìƒìœ„ë¡œ ì˜¬ë¼ê°€ì„œ ì°¾ê¸°
                current_dir = Path(__file__).parent  # diary_mbti
                app_dir = current_dir.parent  # app
                service_dir = app_dir.parent  # diary_service
                business_dir = service_dir.parent  # business
                ai_dir = business_dir.parent  # ai.aiion.site
                common_model_path = ai_dir / "models" / "koelectra"
                if common_model_path.exists() and common_model_path.is_dir() and (common_model_path / "config.json").exists():
                    model_path_str = str(common_model_path)
                    ic(f"âœ… ê³µí†µ ëª¨ë¸ ì €ì¥ì†Œ ì‚¬ìš©: {model_path_str}")
                else:
                    # 3. ê¸°ì¡´ ìœ„ì¹˜ (í•˜ìœ„ í˜¸í™˜ì„±)
                    potential_path = app_dir / model_path_str
                    if potential_path.exists() and potential_path.is_dir() and (potential_path / "config.json").exists():
                        model_path_str = str(potential_path)
                    else:
                        potential_path = ai_dir / model_path_str
                        if potential_path.exists() and potential_path.is_dir() and (potential_path / "config.json").exists():
                            model_path_str = str(potential_path)
        
        model_path = Path(model_path_str)
        is_local_model = model_path.exists() and model_path.is_dir() and (model_path / "config.json").exists()
        
        if is_local_model:
            # ë¡œì»¬ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
            ic(f"âœ… ë¡œì»¬ í† í¬ë‚˜ì´ì € ë¡œë“œ: {model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(str(model_path))
        else:
            # HuggingFace í† í¬ë‚˜ì´ì € ë¡œë“œ
            ic(f"ğŸŒ HuggingFace í† í¬ë‚˜ì´ì € ë¡œë“œ: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # 4ê°œ MBTI ì°¨ì›ë³„ ëª¨ë¸ ì´ˆê¸°í™”
        self.models = {}  # {'E_I': model, 'S_N': model, 'T_F': model, 'J_P': model}
        self.mbti_labels = ['E_I', 'S_N', 'T_F', 'J_P']
        
        ic(f"DiaryMbtiDLModel ì´ˆê¸°í™” ì™„ë£Œ: device={self.device}")
    
    def create_models(
        self,
        num_labels: int = 3,  # MBTI 3-class (0=í‰ê°€ë¶ˆê°€, 1, 2)
        dropout_rate: float = 0.3,
        hidden_size: Optional[int] = None
    ):
        """
        4ê°œ MBTI ì°¨ì›ë³„ ëª¨ë¸ ìƒì„±
        
        Args:
            num_labels: í´ë˜ìŠ¤ ìˆ˜ (MBTIëŠ” 3: 0=í‰ê°€ë¶ˆê°€, 1, 2)
            dropout_rate: Dropout ë¹„ìœ¨
            hidden_size: ì¤‘ê°„ hidden layer í¬ê¸°
        """
        for label in self.mbti_labels:
            self.models[label] = BERTMbtiClassifier(
                model_name=self.model_name,
                num_labels=num_labels,
                dropout_rate=dropout_rate,
                hidden_size=hidden_size
            )
            self.models[label].to(self.device)
        ic(f"4ê°œ MBTI ì°¨ì›ë³„ ëª¨ë¸ ìƒì„± ì™„ë£Œ: {self.model_name} ({num_labels}-class)")
    
    def __repr__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"DiaryMbtiDLModel(model_name={self.model_name}, device={self.device}, models={len(self.models)}ê°œ)"
