"""
Diary MBTI Service
ì¼ê¸° MBTI ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ì„œë¹„ìŠ¤ (DL ì „ìš©)
"""

import sys
from pathlib import Path
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
import pickle
from datetime import datetime

# ic ë¨¼ì € ì •ì˜
try:
    from icecream import ic  # type: ignore
except ImportError:
    def ic(*args, **kwargs):
        if args or kwargs:
            print(*args, **kwargs)
        return args[0] if args else None

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€ (business/diary_service/appì´ ë£¨íŠ¸)
sys.path.insert(0, str(Path(__file__).parent.parent))

from diary_mbti.diary_mbti_dataset import DiaryMbtiDataSet
from diary_mbti.diary_mbti_method import DiaryMbtiMethod

# ë”¥ëŸ¬ë‹ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ
try:
    from diary_mbti.diary_mbti_model import DiaryMbtiDLModel, TORCH_AVAILABLE
    from diary_mbti.diary_mbti_dl_trainer import DiaryMbtiDLTrainer
    DL_AVAILABLE = TORCH_AVAILABLE
except ImportError:
    DL_AVAILABLE = False
    ic("ê²½ê³ : ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class DiaryMbtiService:
    """ì¼ê¸° MBTI ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ì„œë¹„ìŠ¤ (DL ì „ìš©, KoELECTRA ê¸°ë°˜)"""
    
    def __init__(
        self, 
        json_files: Optional[Dict[str, Path]] = None,
        dl_model_name: str = "koelectro_v3_base"  # ë¡œì»¬ KoELECTRA v3 base ëª¨ë¸ ì‚¬ìš©
    ):
        """
        ì´ˆê¸°í™” (DL ì „ìš©, JSON ì „ìš©)
        
        Args:
            json_files: JSON íŒŒì¼ ê²½ë¡œ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)
                       - ë¦¬ìŠ¤íŠ¸: [{'E_I': path, ...}, {'E_I': path, ...}] (ì—¬ëŸ¬ íŒŒì¼ì…‹)
                       - ë”•ì…”ë„ˆë¦¬: {'E_I': path, 'S_N': path, ...} (ë‹¨ì¼ íŒŒì¼ì…‹)
            dl_model_name: ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: koelectro_v3_base)
        """
        self.dataset = DiaryMbtiDataSet()
        self.mbti_labels = ['E_I', 'S_N', 'T_F', 'J_P']  # MBTI 4ê°œ ì°¨ì›
        self.method = DiaryMbtiMethod(self.mbti_labels)  # ì „ì²˜ë¦¬ ë©”ì„œë“œ í´ë˜ìŠ¤
        
        # DL ì „ìš© ì„¤ì •
        self.dl_model_name = dl_model_name
        
        # JSON íŒŒì¼ ê²½ë¡œ (í•„ìˆ˜)
        if json_files is None:
            raise ValueError("json_filesëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. JSON íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µí•˜ì„¸ìš”.")
        self.json_files = json_files
        
        self.df: Optional[pd.DataFrame] = None
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ì¤‘ì•™ ì €ì¥ì†Œ: models/trained_models/diary_mbti/)
        # Docker í™˜ê²½: /app/models/trained_models/diary_mbti
        # ë¡œì»¬ í™˜ê²½: ai.aiion.site/models/trained_models/diary_mbti
        docker_model_dir = Path("/app/models/trained_models/diary_mbti")
        if docker_model_dir.exists():
            self.model_dir = docker_model_dir
            ic(f"âœ… Docker ì¤‘ì•™ ì €ì¥ì†Œ ì‚¬ìš©: {self.model_dir}")
        else:
            # ë¡œì»¬ í™˜ê²½: ìƒëŒ€ ê²½ë¡œë¡œ ì°¾ê¸°
            current_dir = Path(__file__).parent  # diary_mbti
            app_dir = current_dir.parent  # app
            service_dir = app_dir.parent  # diary_service
            business_dir = service_dir.parent  # business
            ai_dir = business_dir.parent  # ai.aiion.site
            local_model_dir = ai_dir / "models" / "trained_models" / "diary_mbti"
            if local_model_dir.exists():
                self.model_dir = local_model_dir
                ic(f"âœ… ë¡œì»¬ ì¤‘ì•™ ì €ì¥ì†Œ ì‚¬ìš©: {self.model_dir}")
            else:
                # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ ìœ„ì¹˜
                self.model_dir = Path(__file__).parent / "models"
                self.model_dir.mkdir(parents=True, exist_ok=True)
                ic(f"âš ï¸ ì¤‘ì•™ ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ì¡´ ìœ„ì¹˜ ì‚¬ìš©: {self.model_dir}")
        
        # DL ëª¨ë¸ íŒŒì¼ (4ê°œ ì°¨ì›ë³„)
        self.dl_model_files = {
            'E_I': self.model_dir / "diary_mbti_e_i_dl_model.pt",
            'S_N': self.model_dir / "diary_mbti_s_n_dl_model.pt",
            'T_F': self.model_dir / "diary_mbti_t_f_dl_model.pt",
            'J_P': self.model_dir / "diary_mbti_j_p_dl_model.pt"
        }
        self.dl_metadata_file = self.model_dir / "diary_mbti_dl_metadata.pkl"
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ
        self.dl_model_obj: Optional[DiaryMbtiDLModel] = None
        self.dl_trainer: Optional[DiaryMbtiDLTrainer] = None
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë¡œê·¸ ì¶œë ¥
        ic(f"ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬: {self.model_dir}")
        ic(f"ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (ì ˆëŒ€ ê²½ë¡œ): {self.model_dir.absolute()}")
        
        ic("DiaryMbtiService ì´ˆê¸°í™”: DL ì „ìš© ëª¨ë“œ")
        
        # DL ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        if not DL_AVAILABLE:
            raise RuntimeError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PyTorchì™€ transformersë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸°í™”
        self._init_dl_model()
        
        # ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„
        self._try_load_model()
    
    def _init_dl_model(self):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸°í™” (DL ì „ìš©)"""
        try:
            self.dl_model_obj = DiaryMbtiDLModel(
                model_name=self.dl_model_name,
                max_length=512
            )
            ic(f"âœ… DL ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.dl_model_name}")
            ic("   MBTIëŠ” 3-class ë¶„ë¥˜: 0=í‰ê°€ë¶ˆê°€, 1=ì²«ë²ˆì§¸, 2=ë‘ë²ˆì§¸")
        except Exception as e:
            ic(f"âŒ DL ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"DL ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_and_merge_json_files(self) -> pd.DataFrame:
        """JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ë³‘í•©í•˜ì—¬ DataFrame ìƒì„± (ì—¬ëŸ¬ íŒŒì¼ì…‹ ì§€ì›)"""
        import json
        
        ic("JSON íŒŒì¼ë“¤ ë¡œë“œ ì¤‘...")
        
        # json_filesê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì—¬ëŸ¬ íŒŒì¼ì…‹)
        if isinstance(self.json_files, list):
            all_dfs = []
            for file_set_idx, file_set in enumerate(self.json_files):
                ic(f"\níŒŒì¼ì…‹ {file_set_idx + 1}/{len(self.json_files)} ë¡œë”© ì¤‘...")
                df = self._load_single_json_fileset(file_set)
                all_dfs.append(df)
                ic(f"  íŒŒì¼ì…‹ {file_set_idx + 1} ì™„ë£Œ: {len(df):,}ê°œ")
            
            # ëª¨ë“  íŒŒì¼ì…‹ ë³‘í•©
            ic("\nëª¨ë“  íŒŒì¼ì…‹ ë³‘í•© ì¤‘...")
            merged_df = pd.concat(all_dfs, ignore_index=True)
            ic(f"ìµœì¢… ë³‘í•© ì™„ë£Œ: {len(merged_df):,}ê°œ (ì´ {len(all_dfs)}ê°œ íŒŒì¼ì…‹)")
            
        else:
            # ë‹¨ì¼ íŒŒì¼ì…‹
            merged_df = self._load_single_json_fileset(self.json_files)
        
        return merged_df
    
    def _load_single_json_fileset(self, json_files: Dict[str, Path]) -> pd.DataFrame:
        """ë‹¨ì¼ JSON íŒŒì¼ì…‹ ë¡œë“œ"""
        import json
        
        # ê° ì°¨ì›ë³„ ë°ì´í„° ë¡œë“œ
        dimension_dfs = {}
        base_data = None
        
        for dimension, json_path in json_files.items():
            ic(f"  [{dimension}] ë¡œë”©: {json_path.name}")
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(data)
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì €ì¥ (id, content, localdate, userid)
            if base_data is None:
                base_data = df[['id', 'content', 'localdate', 'userid']].copy()
                ic(f"     ê¸°ë³¸ ì •ë³´: {len(base_data):,}ê°œ")
            
            # í•´ë‹¹ ì°¨ì›ì˜ ë¼ë²¨ë§Œ ì €ì¥
            dimension_dfs[dimension] = df[['id', dimension]].copy()
            
            # ë¼ë²¨ ë¶„í¬ í™•ì¸
            label_dist = df[dimension].value_counts().to_dict()
            ic(f"     ë¼ë²¨ ë¶„í¬: {label_dist}")
        
        # ëª¨ë“  ì°¨ì› ë³‘í•©
        merged_df = base_data.copy()
        
        for dimension, df in dimension_dfs.items():
            merged_df = merged_df.merge(df, on='id', how='inner')
        
        ic(f"  ë³‘í•© ì™„ë£Œ: {len(merged_df):,}ê°œ")
        
        # title ì»¬ëŸ¼ ì¶”ê°€ (ë¹ˆ ê°’ìœ¼ë¡œ, preprocess_textì—ì„œ contentë§Œ ì‚¬ìš©)
        merged_df['title'] = ''
        
        return merged_df
    
    def preprocess(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ (JSON ì „ìš©)"""
        ic("ğŸ˜ğŸ˜ ì „ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # JSON íŒŒì¼ë“¤ ë¡œë“œ ë° ë³‘í•©
            ic("JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë³‘í•©í•©ë‹ˆë‹¤...")
            self.df = self._load_and_merge_json_files()
            
            # ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
            ic(f"ì»¬ëŸ¼: {list(self.df.columns)}")
            ic(f"ë°ì´í„° íƒ€ì…: {self.df.dtypes.to_dict()}")
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (content, MBTI ë¼ë²¨ë§Œ ì‚¬ìš©)
            required_cols = ['content'] + self.mbti_labels
            ic(f"í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ: {required_cols}")
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (method ì‚¬ìš©)
            self.df = self.method.handle_missing_values(self.df, required_cols)
            
            # MBTI ë¼ë²¨ ë¶„í¬ í™•ì¸ (method ì‚¬ìš©)
            self.method.check_label_distribution(self.df)
            
            # MBTI ë¼ë²¨ ê°’ ê²€ì¦ (ì •ì œëœ ë°ì´í„°ëŠ” 1, 2ë§Œ ìˆì–´ì•¼ í•¨)
            for label in self.mbti_labels:
                unique_values = self.df[label].unique()
                ic(f"{label} ê³ ìœ  ê°’: {sorted(unique_values)}")
                # floatë¥¼ intë¡œ ë³€í™˜
                self.df[label] = self.df[label].astype(int)
            
            # ë¼ë²¨ ê²€ì¦ (0, 1, 2 ëª¨ë‘ ì‚¬ìš© - 3-class ë¶„ë¥˜)
            ic("ë¼ë²¨ ë¶„í¬ í™•ì¸ (0=í‰ê°€ë¶ˆê°€, 1=ì²«ë²ˆì§¸, 2=ë‘ë²ˆì§¸)")
            for label in self.mbti_labels:
                dist = self.df[label].value_counts().to_dict()
                ic(f"  {label}: {dist}")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: title + content ë³‘í•© (method ì‚¬ìš©)
            ic("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: title + content ë³‘í•©")
            self.df = self.method.preprocess_text(self.df)
            ic(f"ë³‘í•©ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì²« 3ê°œ):")
            for i, text in enumerate(self.df['text'].head(3)):
                ic(f"  [{i+1}] {text[:100]}...")
            
            # ë°ì´í„° í’ˆì§ˆ ê°œì„ : ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§ (ìµœì†Œ 50ì, 85% ëª©í‘œ ì‹œ 100ì ê¶Œì¥)
            ic("\në°ì´í„° í’ˆì§ˆ ê°œì„ : ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§")
            # 85% ì •í™•ë„ ëª©í‘œ ì‹œ min_length=100 ê¶Œì¥
            self.df = self.method.filter_short_texts(self.df, min_length=50)
            
            # ë°ì´í„° í’ˆì§ˆ ê°œì„ : í‰ê°€ë¶ˆê°€(0) ë ˆì´ë¸” í•„í„°ë§
            ic("\në°ì´í„° í’ˆì§ˆ ê°œì„ : í‰ê°€ë¶ˆê°€(0) ë ˆì´ë¸” í•„í„°ë§")
            self.df = self.method.filter_zero_labels(self.df, min_zero_ratio=0.3)
            
            # í•„í„°ë§ í›„ ìµœì¢… ë ˆì´ë¸” ë¶„í¬ í™•ì¸
            ic("\ní•„í„°ë§ í›„ ìµœì¢… ë ˆì´ë¸” ë¶„í¬:")
            for label in self.mbti_labels:
                dist = self.df[label].value_counts().to_dict()
                ic(f"  {label}: {dist}")
            
            ic("ğŸ˜ğŸ˜ ì „ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            ic(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise
    
    def learning(
        self,
        epochs: int = 3,
        batch_size: int = 8,
        freeze_bert_layers: int = 8,
        learning_rate: float = 2e-5,
        max_length: int = 512,
        early_stopping_patience: int = 3
    ):
        """
        DL ëª¨ë¸ í•™ìŠµ (4ê°œ MBTI ì°¨ì›ë³„ë¡œ ê°ê° í•™ìŠµ)
        
        Args:
            epochs: ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            freeze_bert_layers: ë™ê²°í•  BERT ë ˆì´ì–´ ìˆ˜
            learning_rate: í•™ìŠµë¥ 
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            early_stopping_patience: Early stopping patience
        """
        ic("ğŸ˜ğŸ˜ DL í•™ìŠµ ì‹œì‘")
        
        try:
            if self.df is None:
                raise ValueError("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ëª¨ë¸ ìƒì„±
            if self.dl_model_obj is None:
                self._init_dl_model()
            
            self.dl_model_obj.create_models(
                num_labels=3,  # MBTI 3-class (0=í‰ê°€ë¶ˆê°€, 1=ì²«ë²ˆì§¸, 2=ë‘ë²ˆì§¸)
                dropout_rate=0.3,
                hidden_size=256
            )
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            from diary_mbti.diary_mbti_dl_trainer import DiaryMbtiDLTrainer
            self.dl_trainer = DiaryMbtiDLTrainer(
                models=self.dl_model_obj.models,
                tokenizer=self.dl_model_obj.tokenizer,
                device=self.dl_model_obj.device
            )
            
            # ë°ì´í„° ì¤€ë¹„
            texts = self.df['text'].tolist()
            
            # 4ê°œ MBTI ì°¨ì›ë³„ ë¼ë²¨ ì¤€ë¹„
            labels_dict = {label: self.df[label].tolist() for label in self.mbti_labels}
            
            # í•™ìŠµ/ê²€ì¦ ë¶„í•  (í…ìŠ¤íŠ¸ëŠ” í•œ ë²ˆë§Œ ë¶„í• , ê° ì°¨ì›ë³„ ë¼ë²¨ì€ ë™ì¼í•œ ì¸ë±ìŠ¤ë¡œ ë¶„í• )
            from sklearn.model_selection import train_test_split
            
            # ì²« ë²ˆì§¸ ì°¨ì›(E_I)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (stratify ì‚¬ìš©)
            train_indices, val_indices, _, _ = train_test_split(
                range(len(texts)), 
                labels_dict['E_I'], 
                test_size=0.2, 
                random_state=42, 
                stratify=labels_dict['E_I']
            )
            
            # í…ìŠ¤íŠ¸ì™€ ê° ì°¨ì›ë³„ ë¼ë²¨ ë¶„í• 
            train_texts = [texts[i] for i in train_indices]
            val_texts = [texts[i] for i in val_indices]
            
            train_labels_dict = {label: [labels_dict[label][i] for i in train_indices] for label in self.mbti_labels}
            val_labels_dict = {label: [labels_dict[label][i] for i in val_indices] for label in self.mbti_labels}
            
            ic(f"í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_texts)}ê°œ")
            
            # í•™ìŠµ (4ê°œ ì°¨ì›ë³„)
            history = self.dl_trainer.train(
                train_texts=train_texts,
                train_labels=train_labels_dict,
                val_texts=val_texts,
                val_labels=val_labels_dict,
                epochs=epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                max_length=max_length,
                freeze_bert_layers=freeze_bert_layers,
                early_stopping_patience=early_stopping_patience,
                use_amp=True
            )
            
            ic(f"í‰ê·  ê²€ì¦ ì •í™•ë„: {history['final_val_accuracy']:.4f}")
            ic("ğŸ˜ğŸ˜ DL í•™ìŠµ ì™„ë£Œ")
            
            return history
            
        except Exception as e:
            ic(f"í•™ìŠµ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def predict(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ MBTI ì˜ˆì¸¡ (4ê°œ ì°¨ì› ëª¨ë‘ ì˜ˆì¸¡) - DL ëª¨ë¸ ì‚¬ìš©"""
        try:
            if self.dl_model_obj is None or not self.dl_model_obj.models:
                raise ValueError("DL ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            import re
            processed_text = str(text)
            processed_text = re.sub(r'\r?\n', ' ', processed_text)
            processed_text = processed_text.replace('\t', ' ')
            processed_text = re.sub(r'\s+', ' ', processed_text).strip()
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì‚¬ì „ ê²€ì‚¬
            text_length = len(processed_text.replace(' ', ''))  # ê³µë°± ì œì™¸ ê¸€ì ìˆ˜
            ic(f"í…ìŠ¤íŠ¸ ê¸¸ì´ (ê³µë°± ì œì™¸): {text_length}ì")
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ í‰ê°€ë¶ˆê°€
            if text_length < 10:
                ic(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({text_length}ì < 10ì): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                return self._create_cannot_evaluate_result("í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
            
            # DL ëª¨ë¸ë¡œ ì˜ˆì¸¡ (4ê°œ ì°¨ì›ë³„)
            predictions = {}
            probabilities = {}
            
            for label in self.mbti_labels:
                model = self.dl_model_obj.models[label]
                model.eval()
                
                # í† í¬ë‚˜ì´ì§•
                encoding = self.dl_model_obj.tokenizer(
                    processed_text,
                    add_special_tokens=True,
                    max_length=512,
                    padding='max_length',
                    truncation=True,
                    return_attention_mask=True,
                    return_tensors='pt'
                )
                
                # ì˜ˆì¸¡
                import torch
                with torch.no_grad():
                    input_ids = encoding['input_ids'].to(self.dl_model_obj.device)
                    attention_mask = encoding['attention_mask'].to(self.dl_model_obj.device)
                    
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    probs = torch.softmax(outputs, dim=1)
                    _, predicted = torch.max(outputs, 1)
                    
                    pred = predicted.cpu().item()  # 0, 1, or 2
                    prob = probs.cpu().numpy()[0]
                    
                    # ë””ë²„ê¹…: ì›ë³¸ í™•ë¥  ë¶„í¬ ì¶œë ¥ (ì¤‘ìš”!)
                    ic(f"[{label}] ì›ë³¸ í™•ë¥ : 0={prob[0]:.4f}, 1={prob[1]:.4f}, 2={prob[2]:.4f} (ì˜ˆì¸¡: {pred})")
                    
                    # ëª¨ë¸ì˜ ì›ë³¸ í™•ë¥ ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í‰ê°€ë¶ˆê°€ íŒë‹¨ì„ ì¡´ì¤‘)
                    adjusted_prob = prob.copy()
                    
                    ic(f"[{label}] ì›ë³¸ í™•ë¥  ê·¸ëŒ€ë¡œ ì‚¬ìš©: 0={prob[0]:.4f}, 1={prob[1]:.4f}, 2={prob[2]:.4f}")
                    
                    # ìµœëŒ€ í™•ë¥ ê³¼ í•´ë‹¹ í´ë˜ìŠ¤ ì°¾ê¸°
                    max_prob_idx = int(np.argmax(adjusted_prob))
                    max_prob = float(adjusted_prob[max_prob_idx])
                    
                    # í™•ë¥  ì„ê³„ê°’ ì„¤ì • (ëª¨ë¸ ì¬í•™ìŠµ ì „ ì„ì‹œ ì¡°ì •)
                    # âš ï¸ ëª¨ë¸ì´ í‰ê°€ë¶ˆê°€ë¥¼ ê³¼ì†Œ ì˜ˆì¸¡(0.2~0.4%)í•˜ê³  ìˆì–´ ì„ê³„ê°’ ëŒ€í­ ë‚®ì¶¤
                    CONFIDENCE_THRESHOLD = 0.55  # MBTI íŒë‹¨ì— í•„ìš”í•œ ì‹ ë¢°ë„ (55% ì´ìƒìœ¼ë¡œ ìƒí–¥)
                    MIN_CONFIDENCE_FOR_EVALUATION = 0.35  # í‰ê°€ ê°€ëŠ¥í•œ ìµœì†Œ í™•ë¥  (35% ì´ìƒ)
                    CANNOT_EVALUATE_THRESHOLD = 0.05  # í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨í•˜ëŠ” ìµœì†Œ í™•ë¥  (5% ì´ìƒ, ëª¨ë¸ ì¬í•™ìŠµ í›„ ìƒí–¥ ì˜ˆì •)
                    CANNOT_EVALUATE_GAP = 0.10  # í‰ê°€ë¶ˆê°€ í™•ë¥ ê³¼ ìµœëŒ€ í™•ë¥ ì˜ ì°¨ì´ê°€ 10% ë¯¸ë§Œì´ë©´ í‰ê°€ë¶ˆê°€
                    
                    # ìµœì¢… ì˜ˆì¸¡ ê²°ì • (ëª¨ë¸ì˜ í‰ê°€ë¶ˆê°€ íŒë‹¨ì„ ì¡´ì¤‘)
                    cannot_evaluate_prob = float(adjusted_prob[0])
                    
                    # í™•ë¥  ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ): [ìµœê³ , ë‘ë²ˆì§¸, í‰ê°€ë¶ˆê°€]
                    sorted_indices = np.argsort(adjusted_prob)[::-1]
                    top_prob = float(adjusted_prob[sorted_indices[0]])
                    second_prob = float(adjusted_prob[sorted_indices[1]]) if len(sorted_indices) > 1 else 0.0
                    
                    # 1. í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ ì„ê³„ê°’ ì´ìƒì´ë©´ í‰ê°€ë¶ˆê°€
                    if cannot_evaluate_prob >= CANNOT_EVALUATE_THRESHOLD:
                        # ì¶”ê°€ ì¡°ê±´: í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ ìµœê³  í™•ë¥ ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë†’ìœ¼ë©´ í™•ì‹¤íˆ í‰ê°€ë¶ˆê°€
                        prob_gap = top_prob - cannot_evaluate_prob
                        if prob_gap < CANNOT_EVALUATE_GAP or cannot_evaluate_prob > top_prob:
                            final_pred = 0
                            ic(f"[{label}] í‰ê°€ë¶ˆê°€ í™•ë¥  ë†’ìŒ ({cannot_evaluate_prob:.3f} >= {CANNOT_EVALUATE_THRESHOLD}, í™•ë¥ ì°¨ì´={prob_gap:.3f}): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                        # í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ ë†’ì§€ë§Œ ë‹¤ë¥¸ í´ë˜ìŠ¤ê°€ ë” ëª…í™•í•˜ë©´ í•´ë‹¹ í´ë˜ìŠ¤ ì„ íƒ
                        elif max_prob_idx != 0 and top_prob >= CONFIDENCE_THRESHOLD and prob_gap >= CANNOT_EVALUATE_GAP:
                            final_pred = max_prob_idx
                            ic(f"[{label}] í‰ê°€ë¶ˆê°€ í™•ë¥  ë†’ì§€ë§Œ ë‹¤ë¥¸ í´ë˜ìŠ¤ê°€ ë” ëª…í™• (í‰ê°€ë¶ˆê°€={cannot_evaluate_prob:.3f}, ìµœê³ ={top_prob:.3f}, ì°¨ì´={prob_gap:.3f}): {final_pred}ë¡œ íŒë‹¨")
                        else:
                            final_pred = 0
                            ic(f"[{label}] í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ ë†’ê³  ë‹¤ë¥¸ í´ë˜ìŠ¤ë„ ë¶ˆëª…í™•: í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                    
                    # 2. í‰ê°€ë¶ˆê°€ê°€ ë‚®ì€ ê²½ìš°, ìµœëŒ€ í™•ë¥ ì´ ì¶©ë¶„íˆ ë†’ê³  í‰ê°€ë¶ˆê°€ í™•ë¥ ê³¼ ì¶©ë¶„í•œ ì°¨ì´ê°€ ìˆì–´ì•¼ íŒë‹¨
                    elif max_prob_idx != 0 and top_prob >= CONFIDENCE_THRESHOLD:
                        # í‰ê°€ë¶ˆê°€ í™•ë¥ ê³¼ì˜ ì°¨ì´ í™•ì¸
                        prob_gap_from_cannot = top_prob - cannot_evaluate_prob
                        if prob_gap_from_cannot >= CANNOT_EVALUATE_GAP:
                            final_pred = max_prob_idx
                            ic(f"[{label}] ì‹ ë¢°ë„ ì¶©ë¶„ ({top_prob:.3f} >= {CONFIDENCE_THRESHOLD}, í‰ê°€ë¶ˆê°€ì™€ ì°¨ì´={prob_gap_from_cannot:.3f}): {final_pred}ë¡œ íŒë‹¨")
                        else:
                            final_pred = 0
                            ic(f"[{label}] ì‹ ë¢°ë„ëŠ” ì¶©ë¶„í•˜ë‚˜ í‰ê°€ë¶ˆê°€ í™•ë¥ ê³¼ ì°¨ì´ê°€ ì‘ìŒ (ìµœê³ ={top_prob:.3f}, í‰ê°€ë¶ˆê°€={cannot_evaluate_prob:.3f}, ì°¨ì´={prob_gap_from_cannot:.3f}): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                    
                    # 3. ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í‰ê°€ë¶ˆê°€
                    else:
                        final_pred = 0
                        ic(f"[{label}] ì‹ ë¢°ë„ ë‚®ìŒ (ìµœëŒ€í™•ë¥ ={top_prob:.3f} < {CONFIDENCE_THRESHOLD}, í‰ê°€ë¶ˆê°€={cannot_evaluate_prob:.3f}): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                    
                    predictions[label] = int(final_pred)
                    probabilities[label] = {
                        '0': float(adjusted_prob[0]),  # í‰ê°€ë¶ˆê°€
                        '1': float(adjusted_prob[1]),  # ì²«ë²ˆì§¸ (E, S, T, J)
                        '2': float(adjusted_prob[2])   # ë‘ë²ˆì§¸ (I, N, F, P)
                    }
                    
                    # í¼ì„¼íŠ¸ ë³€í™˜ (í”„ë¡ íŠ¸ í‘œì‹œìš©)
                    probabilities[label]['0_percent'] = round(float(adjusted_prob[0]) * 100, 1)  # í‰ê°€ë¶ˆê°€ í¼ì„¼íŠ¸
                    probabilities[label]['1_percent'] = round(float(adjusted_prob[1]) * 100, 1)  # ì²«ë²ˆì§¸ í¼ì„¼íŠ¸
                    probabilities[label]['2_percent'] = round(float(adjusted_prob[2]) * 100, 1)  # ë‘ë²ˆì§¸ í¼ì„¼íŠ¸
                    # ì„ íƒëœ í´ë˜ìŠ¤ì˜ í™•ë¥  í¼ì„¼íŠ¸
                    probabilities[label]['selected_percent'] = round(float(adjusted_prob[final_pred]) * 100, 1)
                    
                    # í™•ë¥  ì°¨ì´(í™•ì‹ ë„ ì°¨ì´) ê³„ì‚°: ìµœê³  í™•ë¥  - ë‘ ë²ˆì§¸ í™•ë¥ 
                    sorted_probs = np.sort(adjusted_prob)[::-1]
                    if len(sorted_probs) >= 2:
                        prob_diff = float(sorted_probs[0] - sorted_probs[1])
                        probabilities[label]['confidence_gap'] = prob_diff  # í™•ì‹ ë„ ì°¨ì´ (ë†’ì„ìˆ˜ë¡ í™•ì‹¤)
                        
                        # ì‹¤ì œ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°: ì—”íŠ¸ë¡œí”¼ (ë‚®ì„ìˆ˜ë¡ í™•ì‹¤, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
                        # ì—”íŠ¸ë¡œí”¼ = -Î£(p * log(p)), 0~log(3) ë²”ìœ„, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤
                        entropy = float(-np.sum(adjusted_prob * np.log(adjusted_prob + 1e-10)))  # 1e-10ìœ¼ë¡œ 0 ë‚˜ëˆ” ë°©ì§€
                        max_entropy = np.log(len(adjusted_prob))  # ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ (log(3) â‰ˆ 1.099)
                        normalized_entropy = entropy / max_entropy  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™” (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
                        probabilities[label]['uncertainty'] = normalized_entropy  # ì‹¤ì œ ë¶ˆí™•ì‹¤ì„± (0~1, ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
                        
                        probabilities[label]['confidence'] = float(sorted_probs[0])  # ìµœê³  í™•ë¥  = ì‹ ë¢°ë„
                        probabilities[label]['confidence_percent'] = round(float(sorted_probs[0]) * 100, 1)  # ì‹ ë¢°ë„ í¼ì„¼íŠ¸
                        # ì• ë§¤í•œ ì¼ê¸° íŒë‹¨: í™•ë¥  ì°¨ì´ê°€ 0.1 ë¯¸ë§Œì´ë©´ ì• ë§¤í•¨ (Python boolë¡œ ë³€í™˜)
                        probabilities[label]['is_ambiguous'] = bool(prob_diff < 0.1)
                    else:
                        probabilities[label]['confidence_gap'] = 0.0
                        probabilities[label]['uncertainty'] = 1.0  # í™•ë¥  ì •ë³´ê°€ ì—†ìœ¼ë©´ ìµœëŒ€ ë¶ˆí™•ì‹¤
                        probabilities[label]['confidence'] = float(sorted_probs[0]) if len(sorted_probs) > 0 else 0.0
                        probabilities[label]['confidence_percent'] = round(probabilities[label]['confidence'] * 100, 1)
                        probabilities[label]['is_ambiguous'] = True
                    
                    # ë””ë²„ê¹…: ìµœì¢… ì˜ˆì¸¡ ë° í™•ë¥  ì¶œë ¥ (í•­ìƒ ì¶œë ¥)
                    ic(f"[{label}] ìµœì¢… ì˜ˆì¸¡: {final_pred} (ì¡°ì • í™•ë¥ : 0={adjusted_prob[0]:.4f}, 1={adjusted_prob[1]:.4f}, 2={adjusted_prob[2]:.4f})")
                    ic(f"[{label}] í™•ì‹ ë„ ì°¨ì´: {probabilities[label]['confidence_gap']:.4f}, ë¶ˆí™•ì‹¤ì„±(ì—”íŠ¸ë¡œí”¼): {probabilities[label]['uncertainty']:.4f}, ì‹ ë¢°ë„: {probabilities[label]['confidence']:.4f}, ì• ë§¤í•¨: {probabilities[label]['is_ambiguous']}")
                    
                    # ë””ë²„ê¹…: ì›ë³¸ vs ì¡°ì •ëœ í™•ë¥  ë¹„êµ (í•­ìƒ ì¶œë ¥)
                    if final_pred != pred:
                        ic(f"âš ï¸ [{label}] ì˜ˆì¸¡ ë³€ê²½: {pred} -> {final_pred}")
                    ic(f"[{label}] ì›ë³¸: 0={prob[0]:.4f}, 1={prob[1]:.4f}, 2={prob[2]:.4f} | ì¡°ì •: 0={adjusted_prob[0]:.4f}, 1={adjusted_prob[1]:.4f}, 2={adjusted_prob[2]:.4f} | ì˜ˆì¸¡: {pred}->{final_pred}")
            
            # MBTI ê²°ê³¼ êµ¬ì„± (ê° ì°¨ì› ë…ë¦½ì ìœ¼ë¡œ íŒë‹¨)
            mbti_map = {
                'E_I': {0: '?', 1: 'E', 2: 'I'},
                'S_N': {0: '?', 1: 'S', 2: 'N'},
                'T_F': {0: '?', 1: 'T', 2: 'F'},
                'J_P': {0: '?', 1: 'J', 2: 'P'}
            }
            
            mbti_result = {
                dim: mbti_map[dim].get(predictions.get(dim, 0), '?')
                for dim in self.mbti_labels
            }
            
            full_mbti = ''.join(mbti_result.values())
            
            # ëª¨ë“  ì°¨ì›ì´ í‰ê°€ë¶ˆê°€ì¸ ê²½ìš°ì—ë§Œ "í‰ê°€ë¶ˆê°€"
            if full_mbti == '????':
                full_mbti = 'í‰ê°€ë¶ˆê°€'
            # ê·¸ ì™¸ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥ (ì˜ˆ: E?F?, ENFP, ??T?, ë“±)
            
            # ì „ì²´ MBTI ë¶ˆí™•ì‹¤ì„± ë° ì‹ ë¢°ë„ ê³„ì‚° (í‰ê· )
            total_uncertainty = float(np.mean([probabilities[label].get('uncertainty', 0.0) for label in self.mbti_labels]))
            total_confidence_gap = float(np.mean([probabilities[label].get('confidence_gap', 0.0) for label in self.mbti_labels]))
            total_confidence = float(np.mean([probabilities[label].get('confidence', 0.0) for label in self.mbti_labels]))
            total_confidence_percent = round(total_confidence * 100, 1)  # ì „ì²´ í‰ê·  ì‹ ë¢°ë„ í¼ì„¼íŠ¸
            # ì „ì²´ì ìœ¼ë¡œ ì• ë§¤í•œì§€ íŒë‹¨: ë¶ˆí™•ì‹¤ì„±(ì—”íŠ¸ë¡œí”¼)ì´ 0.3 ì´ìƒì´ë©´ ì• ë§¤í•¨ (ì—”íŠ¸ë¡œí”¼ëŠ” ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
            is_ambiguous_overall = bool(total_uncertainty >= 0.3)  # ì „ì²´ì ìœ¼ë¡œ ì• ë§¤í•œì§€ íŒë‹¨ (Python boolë¡œ ë³€í™˜)
            
            # ì°¨ì›ë³„ ì• ë§¤í•¨ ì •ë³´
            ambiguous_dimensions = [label for label in self.mbti_labels if probabilities[label].get('is_ambiguous', False)]
            
            # ì°¨ì›ë³„ í™•ë¥  ìš”ì•½ (í”„ë¡ íŠ¸ í‘œì‹œìš©)
            dimension_percentages = {}
            for label in self.mbti_labels:
                pred = predictions.get(label, 0)
                dimension_percentages[label] = {
                    'selected': mbti_result[label],  # ì„ íƒëœ ê°’ (E, I, S, N, T, F, J, P, ?)
                    'percent': probabilities[label].get('selected_percent', 0.0),  # ì„ íƒëœ í´ë˜ìŠ¤ì˜ í™•ë¥  í¼ì„¼íŠ¸
                    'confidence_percent': probabilities[label].get('confidence_percent', 0.0)  # ì‹ ë¢°ë„ í¼ì„¼íŠ¸
                }
            
            return {
                'mbti': full_mbti,
                'dimensions': mbti_result,
                'predictions': predictions,
                'probabilities': probabilities,
                'confidence': total_confidence,  # ì „ì²´ í‰ê·  ì‹ ë¢°ë„ (0.0~1.0)
                'confidence_percent': total_confidence_percent,  # ì „ì²´ í‰ê·  ì‹ ë¢°ë„ í¼ì„¼íŠ¸
                'uncertainty': total_uncertainty,  # ì „ì²´ í‰ê·  ë¶ˆí™•ì‹¤ì„±
                'is_ambiguous': is_ambiguous_overall,  # ì „ì²´ì ìœ¼ë¡œ ì• ë§¤í•œì§€ (Python bool)
                'ambiguous_dimensions': ambiguous_dimensions,  # ì• ë§¤í•œ ì°¨ì› ëª©ë¡
                'dimension_percentages': dimension_percentages  # ì°¨ì›ë³„ í™•ë¥  í¼ì„¼íŠ¸ (í”„ë¡ íŠ¸ í‘œì‹œìš©)
            }
            
        except Exception as e:
            ic(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            raise
    
    def _create_cannot_evaluate_result(self, reason: str) -> Dict[str, Any]:
        """
        í‰ê°€ë¶ˆê°€ ê²°ê³¼ ìƒì„± í—¬í¼ ë©”ì„œë“œ
        
        Args:
            reason: í‰ê°€ë¶ˆê°€ ì´ìœ 
        
        Returns:
            í‰ê°€ë¶ˆê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        ic(f"í‰ê°€ë¶ˆê°€ ê²°ê³¼ ìƒì„±: {reason}")
        
        # ëª¨ë“  ì°¨ì›ì„ í‰ê°€ë¶ˆê°€(0)ë¡œ ì„¤ì •
        predictions = {
            'E_I': 0,
            'S_N': 0,
            'T_F': 0,
            'J_P': 0
        }
        
        # í™•ë¥ ë„ í‰ê°€ë¶ˆê°€ì— 100% ì„¤ì •
        probabilities = {}
        dimension_percentages = {}
        for label in ['E_I', 'S_N', 'T_F', 'J_P']:
            prob_data = {
                '0': 1.0,  # í‰ê°€ë¶ˆê°€ 100%
                '1': 0.0,
                '2': 0.0,
                '0_percent': 100.0,
                '1_percent': 0.0,
                '2_percent': 0.0,
                'selected_percent': 100.0,
                'uncertainty': 1.0,  # ë¶ˆí™•ì‹¤ì„± ìµœëŒ€
                'confidence': 1.0,  # í‰ê°€ë¶ˆê°€ì— ëŒ€í•œ ì‹ ë¢°ë„ 100%
                'confidence_percent': 100.0,
                'is_ambiguous': False,  # ëª…í™•í•˜ê²Œ í‰ê°€ë¶ˆê°€
                'selected': '?'
            }
            probabilities[label] = prob_data
            dimension_percentages[label] = prob_data
        
        return {
            'mbti_type': '????',  # í‰ê°€ë¶ˆê°€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ íƒ€ì…
            'E_I': '?',
            'S_N': '?',
            'T_F': '?',
            'J_P': '?',
            'predictions': predictions,
            'probabilities': probabilities,
            'dimension_percentages': dimension_percentages,  # í”„ë¡ íŠ¸ í˜¸í™˜ì„±
            'confidence': 1.0,  # í‰ê°€ë¶ˆê°€ íŒë‹¨ì— ëŒ€í•œ ì‹ ë¢°ë„
            'total_confidence_percent': 100.0,
            'cannot_evaluate': True,
            'cannot_evaluate_reason': reason,
            'model_type': 'dl',
            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def load_model(self) -> bool:
        """DL ëª¨ë¸ ë¡œë“œ (4ê°œ ì°¨ì›ë³„)"""
        try:
            if self.dl_model_obj is None:
                self._init_dl_model()
            
            # ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            all_exist = all(self.dl_model_files[label].exists() for label in self.mbti_labels)
            if not all_exist:
                ic(f"DL ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            if self.dl_metadata_file.exists():
                with open(self.dl_metadata_file, 'rb') as f:
                    metadata = pickle.load(f)
                    dropout_rate = metadata.get('dropout_rate', 0.3)
                    hidden_size = metadata.get('hidden_size', 256)
            else:
                dropout_rate = 0.3
                hidden_size = 256
            
            # ëª¨ë¸ ìƒì„±
            self.dl_model_obj.create_models(
                num_labels=3,  # MBTI 3-class (0=í‰ê°€ë¶ˆê°€, 1=ì²«ë²ˆì§¸, 2=ë‘ë²ˆì§¸)
                dropout_rate=dropout_rate,
                hidden_size=hidden_size
            )
            
            # ê° ì°¨ì›ë³„ ëª¨ë¸ ë¡œë“œ
            import torch
            for label in self.mbti_labels:
                checkpoint = torch.load(
                    self.dl_model_files[label], 
                    map_location=self.dl_model_obj.device
                )
                self.dl_model_obj.models[label].load_state_dict(checkpoint['model_state_dict'])
                self.dl_model_obj.models[label].eval()
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            from diary_mbti.diary_mbti_dl_trainer import DiaryMbtiDLTrainer
            self.dl_trainer = DiaryMbtiDLTrainer(
                models=self.dl_model_obj.models,
                tokenizer=self.dl_model_obj.tokenizer,
                device=self.dl_model_obj.device
            )
            
            ic("DL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (4ê°œ ì°¨ì›)")
            return True
            
        except Exception as e:
            ic(f"DL ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _try_load_model(self):
        """ëª¨ë¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ"""
        try:
            # ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            all_exist = all(self.dl_model_files[label].exists() for label in self.mbti_labels)
            
            if all_exist and self.dl_metadata_file.exists():
                ic("DL ëª¨ë¸ íŒŒì¼ ë°œê²¬, ìë™ ë¡œë“œ ì‹œë„...")
                if self.load_model():
                    ic("DL ëª¨ë¸ ìë™ ë¡œë“œ ì„±ê³µ")
                    return True
                else:
                    ic("DL ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨")
                    return False
            else:
                ic("DL ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return False
        except Exception as e:
            ic(f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def save_model(self):
        """DL ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥ (4ê°œ ì°¨ì›ë³„)"""
        try:
            if self.dl_model_obj is None or not self.dl_model_obj.models:
                raise ValueError("DL ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_dir.mkdir(parents=True, exist_ok=True)
            ic(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.model_dir.absolute()}")
            
            # ê° MBTI ì°¨ì›ë³„ ëª¨ë¸ ì €ì¥ (CPU í˜¸í™˜ í˜•ì‹)
            import torch
            for label in self.mbti_labels:
                if label in self.dl_model_obj.models:
                    model = self.dl_model_obj.models[label]
                    model_path = self.dl_model_files[label]
                    
                    # CPUë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    model_state_dict = model.state_dict()
                    cpu_state_dict = {key: value.cpu() for key, value in model_state_dict.items()}
                    
                    torch.save({
                        'model_state_dict': cpu_state_dict,
                        'model_name': self.dl_model_name,
                        'max_length': self.dl_model_obj.max_length
                    }, model_path)
                    
                    ic(f"âœ… {label} DL ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path} (CPU í˜¸í™˜ í˜•ì‹)")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (dropout_rate, hidden_size í¬í•¨)
            # JSON íŒŒì¼ ì‚¬ìš©ìœ¼ë¡œ csv_mtime ì œê±°
            
            # ëª¨ë¸ì—ì„œ dropout_rateì™€ hidden_size ì¶”ì¶œ
            first_label = self.mbti_labels[0]
            first_model = self.dl_model_obj.models[first_label]
            
            # Dropout rate ì¶”ì¶œ
            dropout_rate = 0.3  # ê¸°ë³¸ê°’
            if hasattr(first_model, 'model') and hasattr(first_model.model, 'dropout'):
                dropout_rate = first_model.model.dropout.p
            
            # Hidden size ì¶”ì¶œ
            hidden_size = None
            if hasattr(first_model, 'model') and hasattr(first_model.model, 'classifier'):
                classifier = first_model.model.classifier
                if isinstance(classifier, torch.nn.Sequential) and len(classifier) > 0:
                    hidden_size = classifier[0].out_features
            
            metadata = {
                'data_source': 'json',
                'trained_at': datetime.now().isoformat(),
                'data_count': len(self.df) if self.df is not None else 0,
                'model_name': self.dl_model_name,
                'mbti_labels': self.mbti_labels,
                'dropout_rate': dropout_rate,
                'hidden_size': hidden_size
            }
            with open(self.dl_metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            ic(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.dl_metadata_file}")
            ic(f"   - dropout_rate: {dropout_rate}")
            ic(f"   - hidden_size: {hidden_size}")
            
        except Exception as e:
            ic(f"ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            raise
    
    def submit(self):
        """ì œì¶œ/ëª¨ë¸ ì €ì¥"""
        ic("ğŸ˜ğŸ˜ ì œì¶œ ì‹œì‘")
        self.save_model()
        ic("ğŸ˜ğŸ˜ ì œì¶œ ì™„ë£Œ")
