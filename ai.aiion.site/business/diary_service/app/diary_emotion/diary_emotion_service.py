"""
Diary Emotion Service
ì¼ê¸° ê°ì • ë¶„ë¥˜ ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤
íŒë‹¤ìŠ¤, ë„˜íŒŒì´, ì‚¬ì´í‚·ëŸ°ì„ ì‚¬ìš©í•œ ë°ì´í„° ì²˜ë¦¬ ë° ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤
"""

import sys
from pathlib import Path
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# hstack ì œê±°ë¨ (Word2Vec ì œê±°ë¡œ ë¶ˆí•„ìš”)

# ic ë¨¼ì € ì •ì˜
try:
    from icecream import ic  # type: ignore
except ImportError:
    def ic(*args, **kwargs):
        if args or kwargs:
            print(*args, **kwargs)
        return args[0] if args else None

# Word2Vec ì œê±°ë¨ - BERTê°€ ë” ìš°ìˆ˜í•œ ë¬¸ë§¥ ì´í•´ë¥¼ ì œê³µí•˜ë¯€ë¡œ ë¶ˆí•„ìš”

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€ (business/diary_service/appì´ ë£¨íŠ¸)
sys.path.insert(0, str(Path(__file__).parent.parent))

from diary_emotion.diary_emotion_dataset import DiaryEmotionDataSet
from diary_emotion.diary_emotion_model import DiaryEmotionModel
from diary_emotion.diary_emotion_method import DiaryEmotionMethod
from diary_emotion.diary_emotion_schema import DiaryEmotionSchema

# ë”¥ëŸ¬ë‹ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ (ì˜µì…˜)
try:
    from diary_emotion.diary_emotion_model import DiaryEmotionDLModel, TORCH_AVAILABLE
    from diary_emotion.diary_emotion_dl_trainer import DiaryEmotionDLTrainer
    DL_AVAILABLE = TORCH_AVAILABLE
except ImportError:
    DL_AVAILABLE = False
    ic("ê²½ê³ : ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class DiaryEmotionService:
    """ì¼ê¸° ê°ì • ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ì„œë¹„ìŠ¤ (DL ì „ìš©)"""
    
    def __init__(
        self,
        csv_file_path: Optional[Path] = None,
        dl_model_name: str = "koelectro_v3_base"  # ë¡œì»¬ KoELECTRA v3 base ëª¨ë¸ ì‚¬ìš©
    ):
        """
        ì´ˆê¸°í™” (DL ì „ìš©)
        
        Args:
            csv_file_path: CSV íŒŒì¼ ê²½ë¡œ
            dl_model_name: ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: koelectro_v3_base)
        """
        self.dataset = DiaryEmotionDataSet()
        self.method = DiaryEmotionMethod()  # ì „ì²˜ë¦¬ ë©”ì„œë“œ í´ë˜ìŠ¤
        
        # DL ì „ìš© ì„¤ì •
        self.model_type = "dl"
        self.dl_model_name = dl_model_name
        
        # CSV íŒŒì¼ ê²½ë¡œ (diary_copers.csv ì‚¬ìš©)
        if csv_file_path is None:
            self.csv_file_path = Path(__file__).parent / "data" / "diary_copers.csv"
        else:
            self.csv_file_path = csv_file_path
        self.df: Optional[pd.DataFrame] = None
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ì¤‘ì•™ ì €ì¥ì†Œ: models/trained_models/diary_emotion/)
        # Docker í™˜ê²½: /app/models/trained_models/diary_emotion
        # ë¡œì»¬ í™˜ê²½: ai.aiion.site/models/trained_models/diary_emotion
        docker_model_dir = Path("/app/models/trained_models/diary_emotion")
        if docker_model_dir.exists():
            self.model_dir = docker_model_dir
            ic(f"âœ… Docker ì¤‘ì•™ ì €ì¥ì†Œ ì‚¬ìš©: {self.model_dir}")
        else:
            # ë¡œì»¬ í™˜ê²½: ìƒëŒ€ ê²½ë¡œë¡œ ì°¾ê¸°
            current_dir = Path(__file__).parent  # diary_emotion
            app_dir = current_dir.parent  # app
            service_dir = app_dir.parent  # diary_service
            business_dir = service_dir.parent  # business
            ai_dir = business_dir.parent  # ai.aiion.site
            local_model_dir = ai_dir / "models" / "trained_models" / "diary_emotion"
            if local_model_dir.exists():
                self.model_dir = local_model_dir
                ic(f"âœ… ë¡œì»¬ ì¤‘ì•™ ì €ì¥ì†Œ ì‚¬ìš©: {self.model_dir}")
            else:
                # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ ìœ„ì¹˜
                self.model_dir = Path(__file__).parent / "models"
                self.model_dir.mkdir(exist_ok=True)
                ic(f"âš ï¸ ì¤‘ì•™ ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ì¡´ ìœ„ì¹˜ ì‚¬ìš©: {self.model_dir}")
        
        # DL ëª¨ë¸ íŒŒì¼
        self.dl_model_file = self.model_dir / "diary_emotion_dl_model.pt"
        self.dl_metadata_file = self.model_dir / "diary_emotion_dl_metadata.pkl"
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ
        self.dl_model_obj: Optional[DiaryEmotionDLModel] = None
        self.dl_trainer: Optional[DiaryEmotionDLTrainer] = None
        
        ic("DiaryEmotionService ì´ˆê¸°í™”: DL ì „ìš© ëª¨ë“œ")
        
        # DL ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        if not DL_AVAILABLE:
            raise RuntimeError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PyTorchì™€ transformersë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸°í™”
        self._init_dl_model()
        
        # ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„
        self._try_load_model()
    
    def _init_dl_model(self):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸°í™” (DL ì „ìš©)"""
        try:
            # ê°ì • í´ë˜ìŠ¤ ìˆ˜ ë™ì  ê³„ì‚° (ë°ì´í„° ë¡œë“œ í›„)
            if self.df is not None and 'emotion' in self.df.columns:
                unique_emotions = self.df['emotion'].unique()
                num_labels = len(unique_emotions)
                ic(f"ê°ì • í´ë˜ìŠ¤ ìˆ˜: {num_labels} (ê°ì • ê°’: {sorted(unique_emotions)})")
            else:
                num_labels = 15  # ê¸°ë³¸ê°’ (ë¡œê·¸ì—ì„œ í™•ì¸ëœ í´ë˜ìŠ¤ ìˆ˜)
                ic(f"ë°ì´í„° ë¯¸ë¡œë“œ, ê¸°ë³¸ ê°ì • í´ë˜ìŠ¤ ìˆ˜ ì‚¬ìš©: {num_labels}")
            
            self.dl_model_obj = DiaryEmotionDLModel(
                model_name=self.dl_model_name,
                num_labels=num_labels,  # ë™ì ìœ¼ë¡œ ê³„ì‚°ëœ ê°ì • í´ë˜ìŠ¤ ìˆ˜
                max_length=512
            )
            ic(f"âœ… DL ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.dl_model_name}")
        except Exception as e:
            ic(f"âŒ DL ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"DL ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def preprocess(self):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        ic("ğŸ˜ğŸ˜ ì „ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # CSV íŒŒì¼ ë¡œë“œ (method ì‚¬ìš©)
            self.df = self.method.load_csv(self.csv_file_path)
            ic(f"CSV íŒŒì¼ ê²½ë¡œ: {self.csv_file_path}")
            ic(f"CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {self.csv_file_path.exists()}")
            
            # ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
            ic(f"ì»¬ëŸ¼: {list(self.df.columns)}")
            ic(f"ë°ì´í„° íƒ€ì…: {self.df.dtypes.to_dict()}")
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (method ì‚¬ìš©)
            # text ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ text ì‚¬ìš©, ì—†ìœ¼ë©´ content ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            required_cols = ['text', 'emotion'] if 'text' in self.df.columns else ['content', 'emotion']
            self.df = self.method.handle_missing_values(self.df, required_cols)
            
            # ê°ì • ë¶„í¬ í™•ì¸
            emotion_dist = self.method.get_label_distribution(self.df, 'emotion')
            if emotion_dist:
                ic(f"ê°ì • ë¶„í¬: {emotion_dist}")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (method ì‚¬ìš©)
            self.df = self.method.preprocess_text(self.df)
            
            # ê°ì • ë¼ë²¨ í™•ì¸ (15ê°œ í´ë˜ìŠ¤)
            emotion_labels_str = "0=í‰ê°€ë¶ˆê°€, 1=ê¸°ì¨, 2=ìŠ¬í””, 3=ë¶„ë…¸, 4=ë‘ë ¤ì›€, 5=í˜ì˜¤, 6=ë†€ëŒ, 7=ì‹ ë¢°, 8=ê¸°ëŒ€, 9=ë¶ˆì•ˆ, 10=ì•ˆë„, 11=í›„íšŒ, 12=ê·¸ë¦¬ì›€, 13=ê°ì‚¬, 14=ì™¸ë¡œì›€"
            ic(f"ê°ì • ë¼ë²¨: {emotion_labels_str}")
            
            ic("ğŸ˜ğŸ˜ ì „ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            ic(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise
    
    def modeling(self):
        """ëª¨ë¸ë§ ì„¤ì •"""
        ic("ğŸ˜ğŸ˜ ëª¨ë¸ë§ ì‹œì‘")
        
        try:
            if self.df is None:
                raise ValueError("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF) - ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ íŒŒë¼ë¯¸í„° ì¡°ì •
            # ë¬¸ë§¥ ì´í•´ë¥¼ ìœ„í•´ ë” ê¸´ n-gram ì‚¬ìš©
            self.model_obj.vectorizer = TfidfVectorizer(
                max_features=10000,  # 5000 -> 10000ìœ¼ë¡œ ì¦ê°€ (ë” ë§ì€ íŠ¹ì§• ì¶”ì¶œ)
                ngram_range=(1, 4),  # (1,3) -> (1,4)ë¡œ ì¦ê°€ (4-gramê¹Œì§€ í¬í•¨, ë¬¸ë§¥ ë” ë§ì´ ë°˜ì˜)
                min_df=1,  # 2 -> 1ë¡œ ê°ì†Œ (ë” ë§ì€ ë‹¨ì–´ í¬í•¨)
                max_df=0.90,  # 0.95 -> 0.90ìœ¼ë¡œ ê°ì†Œ (ë„ˆë¬´ í”í•œ ë‹¨ì–´ ì œê±°)
                sublinear_tf=True  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
            )
            
            # Word2Vec ì œê±°ë¨ - BERTê°€ ë” ìš°ìˆ˜í•œ ë¬¸ë§¥ ì´í•´ë¥¼ ì œê³µ
            # ëª¨ë¸ ì´ˆê¸°í™” (Random Forest) - ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
            self.model_obj.model = RandomForestClassifier(
                n_estimators=200,  # 100 -> 200ìœ¼ë¡œ ì¦ê°€ (ë” ë§ì€ íŠ¸ë¦¬)
                max_depth=30,  # 20 -> 30ìœ¼ë¡œ ì¦ê°€ (ë” ê¹Šì€ íŠ¸ë¦¬)
                min_samples_split=2,  # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                min_samples_leaf=1,  # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
                max_features='sqrt',  # íŠ¹ì§• ì„ íƒ ë°©ì‹
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
            )
            
            ic("ğŸ˜ğŸ˜ ëª¨ë¸ë§ ì™„ë£Œ")
            
        except Exception as e:
            ic(f"ëª¨ë¸ë§ ì˜¤ë¥˜: {e}")
            raise
    
    def learning(
        self, 
        epochs: int = 3, 
        batch_size: int = 8, 
        freeze_bert_layers: int = 8,
        learning_rate: float = 2e-5,
        max_length: int = 256,
        early_stopping_patience: int = 2,
        use_amp: bool = True,
        label_smoothing: float = 0.0  # Label smoothing (0.0 = ë¹„í™œì„±í™”, 0.1 = ê¶Œì¥ê°’)
    ):
        """ëª¨ë¸ í•™ìŠµ (ML ë˜ëŠ” DL)"""
        ic(f"ğŸ˜ğŸ˜ í•™ìŠµ ì‹œì‘: model_type={self.model_type}")
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°
        if self.model_type == "dl":
            return self._learning_dl(
                epochs=epochs, 
                batch_size=batch_size, 
                freeze_bert_layers=freeze_bert_layers,
                learning_rate=learning_rate,
                max_length=max_length,
                early_stopping_patience=early_stopping_patience,
                use_amp=use_amp,
                label_smoothing=label_smoothing
            )
        else:
            return self._learning_ml()
    
    def _learning_ml(self):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ (ê¸°ì¡´)"""
        ic("ğŸ˜ğŸ˜ ML í•™ìŠµ ì‹œì‘")
        
        try:
            if self.df is None:
                raise ValueError("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            if self.model_obj.model is None:
                raise ValueError("ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. modeling()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ìŠ¤íŠ¸ ë²¡í„°í™”
            X_text = self.df['text'].values
            
            # TF-IDF ë²¡í„°í™” (Word2Vec ì œê±°ë¨ - BERTê°€ ë” ìš°ìˆ˜í•œ ë¬¸ë§¥ ì´í•´ ì œê³µ)
            X = self.model_obj.vectorizer.fit_transform(X_text)
            ic(f"TF-IDF ë²¡í„°í™” ì™„ë£Œ: {X.shape}")
            
            # ë¼ë²¨ ì¶”ì¶œ (emotion)
            y = self.df['emotion'].values
            
            # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
            # sparse matrixì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë¶„í• í•˜ê¸° ìœ„í•´ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• 
            indices = list(range(len(y)))
            
            # stratify ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ê° í´ë˜ìŠ¤ê°€ ìµœì†Œ 2ê°œ ì´ìƒì˜ ìƒ˜í”Œ í•„ìš”)
            from collections import Counter
            class_counts = Counter(y)
            min_class_count = min(class_counts.values()) if class_counts else 0
            can_stratify = min_class_count >= 2
            
            if can_stratify:
                ic(f"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜: {dict(class_counts)}, stratify ì‚¬ìš©")
                train_indices, test_indices = train_test_split(
                    indices, test_size=0.2, random_state=42, stratify=y
                )
            else:
                ic(f"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜: {dict(class_counts)}, stratify ì‚¬ìš© ë¶ˆê°€ (ìµœì†Œ ìƒ˜í”Œ ìˆ˜: {min_class_count})")
                train_indices, test_indices = train_test_split(
                    indices, test_size=0.2, random_state=42
                )
            
            # sparse matrixë¥¼ ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¡œ ì¸ë±ì‹±
            X_train = X[train_indices]
            X_test = X[test_indices]
            y_train = y[train_indices]
            y_test = y[test_indices]
            
            # ëª¨ë¸ í•™ìŠµ
            self.model_obj.model.fit(X_train, y_train)
            
            # í•™ìŠµ ë°ì´í„°ì…‹ ì €ì¥
            self.dataset.train = pd.DataFrame({
                'text': self.df['text'].iloc[train_indices].values,
                'emotion': y_train
            })
            self.dataset.test = pd.DataFrame({
                'text': self.df['text'].iloc[test_indices].values,
                'emotion': y_test
            })
            
            ic(f"í•™ìŠµ ë°ì´í„°: {X_train.shape[0]} ê°œ")
            ic(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]} ê°œ")
            ic("ğŸ˜ğŸ˜ ML í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            ic(f"ML í•™ìŠµ ì˜¤ë¥˜: {e}")
            raise
    
    def _learning_dl(
        self, 
        epochs: int = 3, 
        batch_size: int = 8, 
        freeze_bert_layers: int = 8,
        learning_rate: float = 2e-5,
        max_length: int = 256,
        early_stopping_patience: int = 2,
        use_amp: bool = True,
        label_smoothing: float = 0.0  # Label smoothing (0.0 = ë¹„í™œì„±í™”, 0.1 = ê¶Œì¥ê°’)
    ):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
        ic("ğŸ˜ğŸ˜ DL í•™ìŠµ ì‹œì‘")
        
        try:
            if not DL_AVAILABLE:
                raise ImportError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if self.df is None:
                raise ValueError("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ëª¨ë¸ ìƒì„±
            if self.dl_model_obj is None:
                self._init_dl_model()
            
            self.dl_model_obj.create_model(
                dropout_rate=0.3,
                hidden_size=256  # ì¤‘ê°„ ë ˆì´ì–´ ì¶”ê°€
            )
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            self.dl_trainer = DiaryEmotionDLTrainer(
                model=self.dl_model_obj.model,
                tokenizer=self.dl_model_obj.tokenizer,
                device=self.dl_model_obj.device
            )
            
            # ë°ì´í„° ì¤€ë¹„
            texts = self.df['text'].tolist()
            labels = self.df['emotion'].tolist()
            
            # í•™ìŠµ/ê²€ì¦ ë¶„í• 
            from sklearn.model_selection import train_test_split
            train_texts, val_texts, train_labels, val_labels = train_test_split(
                texts, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            ic(f"í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_texts)}ê°œ")
            
            # í•™ìŠµ (íŒŒë¼ë¯¸í„° ì „ë‹¬)
            history = self.dl_trainer.train(
                train_texts=train_texts,
                train_labels=train_labels,
                val_texts=val_texts,
                val_labels=val_labels,
                epochs=epochs,
                batch_size=batch_size,
                learning_rate=learning_rate,
                max_length=max_length,
                freeze_bert_layers=freeze_bert_layers,
                early_stopping_patience=early_stopping_patience,
                use_amp=use_amp,
                label_smoothing=label_smoothing
            )
            
            # í•™ìŠµ ë°ì´í„°ì…‹ ì €ì¥
            self.dataset.train = pd.DataFrame({
                'text': train_texts,
                'emotion': train_labels
            })
            self.dataset.test = pd.DataFrame({
                'text': val_texts,
                'emotion': val_labels
            })
            
            ic(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {history['final_val_accuracy']:.4f}")
            ic("ğŸ˜ğŸ˜ DL í•™ìŠµ ì™„ë£Œ")
            
            return history
            
        except Exception as e:
            ic(f"DL í•™ìŠµ ì˜¤ë¥˜: {e}")
            raise
    
    def evaluate(self, model_type: Optional[str] = None):
        """ëª¨ë¸ í‰ê°€ (ML ë˜ëŠ” DL)"""
        # model_typeì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì¸ìŠ¤í„´ìŠ¤ì˜ model_type ì‚¬ìš©
        eval_model_type = model_type or self.model_type
        
        if eval_model_type == "dl":
            return self._evaluate_dl()
        else:
            return self._evaluate_ml()
    
    def _evaluate_ml(self):
        """ML ëª¨ë¸ í‰ê°€"""
        ic("ğŸ˜ğŸ˜ ML í‰ê°€ ì‹œì‘")
        
        try:
            if self.model_obj.model is None:
                raise ValueError("ML ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì¬ìƒì„±
            if self.dataset.test is None:
                ic("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì—†ì–´ì„œ ìë™ìœ¼ë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                if self.df is None:
                    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²˜ë¦¬ë¶€í„° ì‹¤í–‰
                    self.preprocess()
                
                # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  ì¬ìƒì„± (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                texts = self.df['text'].values
                labels = self.df['emotion'].values
                
                from collections import Counter
                class_counts = Counter(labels)
                min_class_count = min(class_counts.values()) if class_counts else 0
                can_stratify = min_class_count >= 2
                
                indices = list(range(len(labels)))
                if can_stratify:
                    train_indices, test_indices = train_test_split(
                        indices, test_size=0.2, random_state=42, stratify=labels
                    )
                else:
                    train_indices, test_indices = train_test_split(
                        indices, test_size=0.2, random_state=42
                    )
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ë§Œ ì €ì¥ (í‰ê°€ì— í•„ìš”)
                self.dataset.test = pd.DataFrame({
                    'text': self.df['text'].iloc[test_indices].values,
                    'emotion': labels[test_indices]
                })
                ic(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì™„ë£Œ: {len(self.dataset.test)}ê°œ")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            X_test_text = self.dataset.test['text'].values
            X_test_tfidf = self.model_obj.vectorizer.transform(X_test_text)
            
            # TF-IDFë§Œ ì‚¬ìš© (Word2Vec ì œê±°ë¨)
            X_test = X_test_tfidf
            y_test = self.dataset.test['emotion'].values
            
            # ì˜ˆì¸¡
            y_pred = self.model_obj.model.predict(X_test)
            
            # ì •í™•ë„ ê³„ì‚°
            accuracy = accuracy_score(y_test, y_pred)
            ic(f"ML ì •í™•ë„: {accuracy:.4f}")
            
            # ë¶„ë¥˜ ë³´ê³ ì„œ
            emotion_labels = {
                0: 'í‰ê°€ë¶ˆê°€', 1: 'ê¸°ì¨', 2: 'ìŠ¬í””', 3: 'ë¶„ë…¸', 4: 'ë‘ë ¤ì›€', 5: 'í˜ì˜¤', 6: 'ë†€ëŒ',
                7: 'ì‹ ë¢°', 8: 'ê¸°ëŒ€', 9: 'ë¶ˆì•ˆ', 10: 'ì•ˆë„', 11: 'í›„íšŒ', 12: 'ê·¸ë¦¬ì›€', 13: 'ê°ì‚¬', 14: 'ì™¸ë¡œì›€'
            }
            # ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            unique_classes = sorted(set(list(y_test) + list(y_pred)))
            target_names = [emotion_labels.get(i, f'í´ë˜ìŠ¤{i}') for i in unique_classes]
            report = classification_report(
                y_test, y_pred,
                target_names=target_names,
                output_dict=True,
                zero_division=0
            )
            ic(f"ML ë¶„ë¥˜ ë³´ê³ ì„œ:\n{classification_report(y_test, y_pred, target_names=target_names, zero_division=0)}")
            
            # í˜¼ë™ í–‰ë ¬
            cm = confusion_matrix(y_test, y_pred)
            ic(f"ML í˜¼ë™ í–‰ë ¬:\n{cm}")
            
            ic("ğŸ˜ğŸ˜ ML í‰ê°€ ì™„ë£Œ")
            
            return {
                'model_type': 'ml',
                'accuracy': accuracy,
                'classification_report': report,
                'confusion_matrix': cm.tolist()
            }
            
        except Exception as e:
            ic(f"ML í‰ê°€ ì˜¤ë¥˜: {e}")
            raise
    
    def _evaluate_dl(self):
        """DL ëª¨ë¸ í‰ê°€"""
        ic("ğŸ˜ğŸ˜ DL í‰ê°€ ì‹œì‘")
        
        try:
            if not DL_AVAILABLE:
                raise ImportError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if self.dl_model_obj is None or self.dl_model_obj.model is None:
                raise ValueError("DL ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì¬ìƒì„±
            if self.dataset.test is None:
                ic("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì—†ì–´ì„œ ìë™ìœ¼ë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                if self.df is None:
                    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²˜ë¦¬ë¶€í„° ì‹¤í–‰
                    self.preprocess()
                
                # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  ì¬ìƒì„± (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                texts = self.df['text'].tolist()
                labels = self.df['emotion'].tolist()
                
                train_texts, val_texts, train_labels, val_labels = train_test_split(
                    texts, labels, test_size=0.2, random_state=42, stratify=labels
                )
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ë§Œ ì €ì¥ (í‰ê°€ì— í•„ìš”)
                self.dataset.test = pd.DataFrame({
                    'text': val_texts,
                    'emotion': val_labels
                })
                ic(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì™„ë£Œ: {len(self.dataset.test)}ê°œ")
            
            # íŠ¸ë ˆì´ë„ˆê°€ ì—†ìœ¼ë©´ ìƒì„±
            if self.dl_trainer is None:
                self.dl_trainer = DiaryEmotionDLTrainer(
                    model=self.dl_model_obj.model,
                    tokenizer=self.dl_model_obj.tokenizer,
                    device=self.dl_model_obj.device
                )
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            test_texts = self.dataset.test['text'].tolist()
            test_labels = self.dataset.test['emotion'].tolist()
            
            # DataLoader ìƒì„±
            from torch.utils.data import DataLoader
            from diary_emotion.diary_emotion_dl_trainer import EmotionDataset
            
            test_dataset = EmotionDataset(
                texts=test_texts,
                labels=test_labels,
                tokenizer=self.dl_model_obj.tokenizer,
                max_length=256
            )
            test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
            
            # ì†ì‹¤ í•¨ìˆ˜
            import torch.nn as nn
            criterion = nn.CrossEntropyLoss()
            
            # í‰ê°€ ì‹¤í–‰
            avg_loss, accuracy, y_true, y_pred = self.dl_trainer.evaluate(test_loader, criterion)
            
            ic(f"DL ì •í™•ë„: {accuracy:.4f}, í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
            
            # ë¶„ë¥˜ ë³´ê³ ì„œ
            emotion_labels = {
                0: 'í‰ê°€ë¶ˆê°€', 1: 'ê¸°ì¨', 2: 'ìŠ¬í””', 3: 'ë¶„ë…¸', 4: 'ë‘ë ¤ì›€', 5: 'í˜ì˜¤', 6: 'ë†€ëŒ',
                7: 'ì‹ ë¢°', 8: 'ê¸°ëŒ€', 9: 'ë¶ˆì•ˆ', 10: 'ì•ˆë„', 11: 'í›„íšŒ', 12: 'ê·¸ë¦¬ì›€', 13: 'ê°ì‚¬', 14: 'ì™¸ë¡œì›€'
            }
            unique_classes = sorted(set(list(y_true) + list(y_pred)))
            target_names = [emotion_labels.get(i, f'í´ë˜ìŠ¤{i}') for i in unique_classes]
            report = classification_report(
                y_true, y_pred,
                target_names=target_names,
                output_dict=True,
                zero_division=0
            )
            ic(f"DL ë¶„ë¥˜ ë³´ê³ ì„œ:\n{classification_report(y_true, y_pred, target_names=target_names, zero_division=0)}")
            
            # í˜¼ë™ í–‰ë ¬
            cm = confusion_matrix(y_true, y_pred)
            ic(f"DL í˜¼ë™ í–‰ë ¬:\n{cm}")
            
            ic("ğŸ˜ğŸ˜ DL í‰ê°€ ì™„ë£Œ")
            
            return {
                'model_type': 'dl',
                'accuracy': float(accuracy),
                'avg_loss': float(avg_loss),
                'classification_report': report,
                'confusion_matrix': cm.tolist()
            }
            
        except Exception as e:
            ic(f"DL í‰ê°€ ì˜¤ë¥˜: {e}")
            raise
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ê°ì • ì˜ˆì¸¡ (DL ì „ìš©)
        
        Args:
            text: ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        return self._predict_dl(text)
    
    def _predict_ml(self, text: str) -> Dict[str, Any]:
        """ML ëª¨ë¸ ì˜ˆì¸¡ (ê¸°ì¡´)"""
        try:
            if self.model_obj.model is None:
                raise ValueError("ML ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì¤„ë°”ê¿ˆ, íƒ­ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì—°ì† ê³µë°± í†µí•©)
            import re
            processed_text = str(text)
            # ì¤„ë°”ê¿ˆ(\n, \r\n)ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
            processed_text = re.sub(r'\r?\n', ' ', processed_text)
            # íƒ­ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
            processed_text = processed_text.replace('\t', ' ')
            # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
            processed_text = re.sub(r'\s+', ' ', processed_text).strip()
            
            # TF-IDF ë²¡í„°í™”
            X_tfidf = self.model_obj.vectorizer.transform([processed_text])
            
            # TF-IDFë§Œ ì‚¬ìš© (Word2Vec ì œê±°ë¨)
            X = X_tfidf
            
            # ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
            prediction = self.model_obj.model.predict(X)[0]
            probabilities = self.model_obj.model.predict_proba(X)[0]
            
            emotion_labels = {
                0: 'í‰ê°€ë¶ˆê°€', 1: 'ê¸°ì¨', 2: 'ìŠ¬í””', 3: 'ë¶„ë…¸', 4: 'ë‘ë ¤ì›€', 5: 'í˜ì˜¤', 6: 'ë†€ëŒ',
                7: 'ì‹ ë¢°', 8: 'ê¸°ëŒ€', 9: 'ë¶ˆì•ˆ', 10: 'ì•ˆë„', 11: 'í›„íšŒ', 12: 'ê·¸ë¦¬ì›€', 13: 'ê°ì‚¬', 14: 'ì™¸ë¡œì›€'
            }
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ë³´ì •
            probabilities = self._apply_keyword_weights(text, probabilities, emotion_labels)
            
            # í‰ê°€ë¶ˆê°€ í™•ë¥  0.84ë°°ë¡œ ì¡°ì • (16% ê°ì†Œ)
            if len(probabilities) > 0:
                probabilities[0] = probabilities[0] * 0.84
                # ì •ê·œí™”
                probabilities = probabilities / (probabilities.sum() + 1e-10)
                ic(f"í‰ê°€ë¶ˆê°€ í™•ë¥  0.84ë°°ë¡œ ì¡°ì • (16% ê°ì†Œ)")
            
            # í‰ê°€ë¶ˆê°€ í™•ë¥  ì¶”ê°€ ê°ì†Œ: ë‹¤ë¥¸ ê°ì •ì˜ í™•ë¥ ì´ ë†’ìœ¼ë©´ í‰ê°€ë¶ˆê°€ í™•ë¥ ì„ ë” ë‚®ì¶¤
            if len(probabilities) > 0:
                # í‰ê°€ë¶ˆê°€(0ë²ˆ)ë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ê°ì •ì˜ ìµœëŒ€ í™•ë¥ 
                other_emotions_probs = probabilities[1:] if len(probabilities) > 1 else []
                if len(other_emotions_probs) > 0:
                    max_other_emotion_prob = float(np.max(other_emotions_probs))
                    cannot_evaluate_prob = float(probabilities[0])
                    
                    # ë‹¤ë¥¸ ê°ì •ì˜ ìµœëŒ€ í™•ë¥ ì´ í‰ê°€ë¶ˆê°€ í™•ë¥ ë³´ë‹¤ ë†’ê±°ë‚˜ ë¹„ìŠ·í•˜ë©´ í‰ê°€ë¶ˆê°€ í™•ë¥  ê°ì†Œ
                    if max_other_emotion_prob >= cannot_evaluate_prob * 0.8:
                        # í‰ê°€ë¶ˆê°€ í™•ë¥ ì„ 15% ê°ì†Œ
                        probabilities[0] = probabilities[0] * 0.85
                        # ì •ê·œí™”
                        probabilities = probabilities / (probabilities.sum() + 1e-10)
                        ic(f"ë‹¤ë¥¸ ê°ì • í™•ë¥ ì´ ë†’ìŒ ({max_other_emotion_prob:.3f} vs {cannot_evaluate_prob:.3f}), í‰ê°€ë¶ˆê°€ í™•ë¥  15% ê°ì†Œ")
            
            # ìŠ¬í”” > í‰ë²” > ê¸°ì¨ ìˆœì„œ ì¡°ì •: ìŠ¬í”” í™•ë¥  ì¦ê°€, ê¸°ì¨ í™•ë¥  ê°ì†Œ (ë¯¸ì„¸ ì¡°ì •)
            if len(probabilities) > 2:
                sadness_prob = float(probabilities[2])  # ìŠ¬í”” (ì¸ë±ìŠ¤ 2)
                joy_prob = float(probabilities[1])      # ê¸°ì¨ (ì¸ë±ìŠ¤ 1)
                ordinary_prob = float(probabilities[0]) # í‰ë²”/í‰ê°€ë¶ˆê°€ (ì¸ë±ìŠ¤ 0)
                
                # ìŠ¬í””ê³¼ ê¸°ì¨ì´ ëª¨ë‘ ì¼ì • í™•ë¥  ì´ìƒì¼ ë•Œ ë¯¸ì„¸ ì¡°ì • (ì¡°ê±´: 20% ì´ìƒ)
                if sadness_prob > 0.20 and joy_prob > 0.20:
                    # ìŠ¬í”” í™•ë¥  8% ì¦ê°€ (ë¯¸ì„¸ ì¡°ì • ê°•í™”)
                    probabilities[2] = sadness_prob * 1.08
                    # ê¸°ì¨ í™•ë¥  8% ê°ì†Œ (ë¯¸ì„¸ ì¡°ì • ê°•í™”)
                    probabilities[1] = joy_prob * 0.92
                    # ì •ê·œí™”
                    probabilities = probabilities / (probabilities.sum() + 1e-10)
                    ic(f"ìŠ¬í””/ê¸°ì¨ í™•ë¥  ë¯¸ì„¸ ì¡°ì •: ìŠ¬í”” {sadness_prob:.3f} -> {probabilities[2]:.3f}, ê¸°ì¨ {joy_prob:.3f} -> {probabilities[1]:.3f}")
                
                # ìŠ¬í””ì´ ê¸°ì¨ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •
                if probabilities[2] < probabilities[1]:
                    # ìŠ¬í””ê³¼ ê¸°ì¨ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •
                    diff = probabilities[1] - probabilities[2]
                    if diff > 0.005:  # ì°¨ì´ê°€ 0.5% ì´ìƒì´ë©´ (ì¡°ê±´ ì™„í™”)
                        # ìŠ¬í”” í™•ë¥  ì¶”ê°€ ë¯¸ì„¸ ì¦ê°€ (5%)
                        probabilities[2] = probabilities[2] * 1.05
                        # ê¸°ì¨ í™•ë¥  ì¶”ê°€ ë¯¸ì„¸ ê°ì†Œ (5%)
                        probabilities[1] = probabilities[1] * 0.95
                        # ì •ê·œí™”
                        probabilities = probabilities / (probabilities.sum() + 1e-10)
                        ic(f"ìŠ¬í”” < ê¸°ì¨ ìˆœì„œ ì—­ì „ ë°©ì§€ (ë¯¸ì„¸ ì¡°ì •): ìŠ¬í”” {probabilities[2]:.3f}, ê¸°ì¨ {probabilities[1]:.3f}")
                
                # ìŠ¬í””ì´ í‰ë²”ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¶”ê°€ ì¡°ì •
                if probabilities[2] < probabilities[0]:
                    # ìŠ¬í”” í™•ë¥ ì„ í‰ë²”ë³´ë‹¤ ì•½ê°„ ë†’ê²Œ ì¡°ì •
                    diff = probabilities[0] - probabilities[2]
                    if diff > 0.005:  # ì°¨ì´ê°€ 0.5% ì´ìƒì´ë©´
                        # ìŠ¬í”” í™•ë¥  ì¶”ê°€ ë¯¸ì„¸ ì¦ê°€ (3%)
                        probabilities[2] = probabilities[2] * 1.03
                        # ì •ê·œí™”
                        probabilities = probabilities / (probabilities.sum() + 1e-10)
                        ic(f"ìŠ¬í”” < í‰ë²” ìˆœì„œ ì—­ì „ ë°©ì§€ (ë¯¸ì„¸ ì¡°ì •): ìŠ¬í”” {probabilities[2]:.3f}, í‰ë²” {probabilities[0]:.3f}")
            
            # ê¸ì • ê°ì • í™•ë¥  0.02ì”© ì¦ê°€ (ê¸°ì¨, ê°ì‚¬, ì‹ ë¢°, ê¸°ëŒ€, ì•ˆë„)
            if len(probabilities) > 14:
                # ê¸°ì¨ (1)
                if probabilities[1] > 0:
                    joy_prob_before = float(probabilities[1])
                    probabilities[1] = probabilities[1] + 0.02
                    ic(f"ê¸°ì¨ í™•ë¥  0.02 ì¦ê°€: {joy_prob_before:.3f} -> {probabilities[1]:.3f}")
                
                # ê°ì‚¬ (13)
                if probabilities[13] > 0:
                    gratitude_prob_before = float(probabilities[13])
                    probabilities[13] = probabilities[13] + 0.02
                    ic(f"ê°ì‚¬ í™•ë¥  0.02 ì¦ê°€: {gratitude_prob_before:.3f} -> {probabilities[13]:.3f}")
                
                # ì‹ ë¢° (7)
                if probabilities[7] > 0:
                    trust_prob_before = float(probabilities[7])
                    probabilities[7] = probabilities[7] + 0.02
                    ic(f"ì‹ ë¢° í™•ë¥  0.02 ì¦ê°€: {trust_prob_before:.3f} -> {probabilities[7]:.3f}")
                
                # ê¸°ëŒ€ (8)
                if probabilities[8] > 0:
                    expectation_prob_before = float(probabilities[8])
                    probabilities[8] = probabilities[8] + 0.02
                    ic(f"ê¸°ëŒ€ í™•ë¥  0.02 ì¦ê°€: {expectation_prob_before:.3f} -> {probabilities[8]:.3f}")
                
                # ì•ˆë„ (10)
                if probabilities[10] > 0:
                    relief_prob_before = float(probabilities[10])
                    probabilities[10] = probabilities[10] + 0.02
                    ic(f"ì•ˆë„ í™•ë¥  0.02 ì¦ê°€: {relief_prob_before:.3f} -> {probabilities[10]:.3f}")
                
                # ì •ê·œí™”
                probabilities = probabilities / (probabilities.sum() + 1e-10)
                ic(f"ê¸ì • ê°ì • í™•ë¥  ë³´ì • ì™„ë£Œ ë° ì •ê·œí™”")
            
            # ìµœëŒ€ í™•ë¥ ê³¼ í•´ë‹¹ í´ë˜ìŠ¤ ì°¾ê¸°
            max_prob_idx = int(np.argmax(probabilities))
            max_prob = float(probabilities[max_prob_idx])
            
            # í‰ê°€ë¶ˆê°€ í™•ë¥  í™•ì¸
            cannot_evaluate_prob = float(probabilities[0]) if len(probabilities) > 0 else 0.0
            
            # í™•ë¥  ì„ê³„ê°’ ì„¤ì •
            CONFIDENCE_THRESHOLD = 0.3
            MIN_CONFIDENCE_FOR_EVALUATION = 0.15  # í‰ê°€ ê°€ëŠ¥í•œ ìµœì†Œ í™•ë¥  (15% ì´ìƒì´ë©´ í‰ê°€ ê°€ëŠ¥)
            CANNOT_EVALUATE_THRESHOLD = 0.5  # í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨í•˜ëŠ” ìµœì†Œ í™•ë¥  (50% ì´ìƒì´ì–´ì•¼ í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨)
            
            # 1. ìµœëŒ€ í™•ë¥ ì´ í‰ê°€ë¶ˆê°€(0)ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if max_prob_idx == 0:
                # í‰ê°€ë¶ˆê°€ê°€ ê°€ì¥ ë†’ì€ í™•ë¥ ì´ì§€ë§Œ, í™•ë¥ ì´ ì¶©ë¶„íˆ ë†’ì•„ì•¼ë§Œ í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨
                if max_prob >= CANNOT_EVALUATE_THRESHOLD:
                    # í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ 50% ì´ìƒì´ë©´ í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨
                    final_prediction = 0
                    final_label = 'í‰ê°€ë¶ˆê°€'
                    ic(f"í‰ê°€ë¶ˆê°€ê°€ ìµœëŒ€ í™•ë¥  ({max_prob:.3f})ì´ê³  ì„ê³„ê°’({CANNOT_EVALUATE_THRESHOLD}) ì´ìƒ: í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                else:
                    # í‰ê°€ë¶ˆê°€ í™•ë¥ ì´ ë‚®ìœ¼ë©´ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ê°ì • í™•ì¸
                    sorted_indices = np.argsort(probabilities)[::-1]
                    if len(sorted_indices) > 1 and len(probabilities) > 1:
                        second_max_idx = int(sorted_indices[1])
                        if 0 <= second_max_idx < len(probabilities):
                            second_max_prob = float(probabilities[second_max_idx])
                            # ë‘ ë²ˆì§¸ ê°ì •ì˜ í™•ë¥ ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒì´ë©´ ê·¸ ê°ì • ì„ íƒ
                            if second_max_prob >= MIN_CONFIDENCE_FOR_EVALUATION and second_max_idx != 0:
                                final_prediction = second_max_idx
                                final_label = emotion_labels.get(second_max_idx, f'í´ë˜ìŠ¤{second_max_idx}')
                                ic(f"í‰ê°€ë¶ˆê°€ í™•ë¥  ë‚®ìŒ ({max_prob:.3f}), ë‘ ë²ˆì§¸ ê°ì • ì„ íƒ: {final_label} ({second_max_prob:.3f})")
                            else:
                                # ë‘ ë²ˆì§¸ ê°ì •ë„ í™•ë¥ ì´ ë‚®ìœ¼ë©´ í‰ê°€ë¶ˆê°€
                                final_prediction = 0
                                final_label = 'í‰ê°€ë¶ˆê°€'
                                ic(f"í‰ê°€ë¶ˆê°€ê°€ ìµœëŒ€ í™•ë¥ ì´ì§€ë§Œ ë‚®ìŒ ({max_prob:.3f}), ë‹¤ë¥¸ ê°ì •ë„ ë‚®ìŒ: í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
                    else:
                        final_prediction = 0
                        final_label = 'í‰ê°€ë¶ˆê°€'
                        ic(f"í‰ê°€ë¶ˆê°€ê°€ ìµœëŒ€ í™•ë¥  ({max_prob:.3f}): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
            # 2. ìµœëŒ€ í™•ë¥ ì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš© (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            elif max_prob >= CONFIDENCE_THRESHOLD:
                final_prediction = max_prob_idx
                final_label = emotion_labels.get(max_prob_idx, 'ì•Œ ìˆ˜ ì—†ìŒ')
                ic(f"ìµœëŒ€ í™•ë¥  ì¶©ë¶„ ({max_prob:.3f}): {final_label}ë¡œ íŒë‹¨")
            # 3. ìµœëŒ€ í™•ë¥ ì´ ë‚®ì€ ê²½ìš°ì—ë„ ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš© (ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¡œ íŒë‹¨)
            elif max_prob >= MIN_CONFIDENCE_FOR_EVALUATION:
                # ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš© (ìµœëŒ€ í™•ë¥ ì´ 15% ì´ìƒì´ë©´)
                final_prediction = max_prob_idx
                final_label = emotion_labels.get(max_prob_idx, 'ì•Œ ìˆ˜ ì—†ìŒ')
                ic(f"ëª¨ë¸ ì˜ˆì¸¡ ì‚¬ìš©: {final_label} ({max_prob:.3f})")
            # 4. í™•ë¥ ì´ ë§¤ìš° ë‚®ìœ¼ë©´ í‰ê°€ë¶ˆê°€
            else:
                # í™•ë¥ ì´ ë§¤ìš° ë‚®ìœ¼ë©´ í‰ê°€ë¶ˆê°€
                final_prediction = 0
                final_label = 'í‰ê°€ë¶ˆê°€'
                ic(f"í™•ë¥  ë§¤ìš° ë‚®ìŒ ({max_prob:.3f}): í‰ê°€ë¶ˆê°€ë¡œ íŒë‹¨")
            
            # í™•ë¥  ì •ë³´ êµ¬ì„± (ìƒìœ„ ê°ì •ë“¤ì— ì§‘ì¤‘í•˜ì—¬ í™•ë¥ ì„ ë” ëª…í™•í•˜ê²Œ í‘œì‹œ)
            # ìƒìœ„ 3ê°œ ê°ì •ì˜ í™•ë¥ ì„ ì¶”ì¶œí•˜ê³  ì¬ë¶„ë°°
            top_3_indices = np.argsort(probabilities)[::-1][:3]  # ìƒìœ„ 3ê°œ ì¸ë±ìŠ¤
            top_3_probs = probabilities[top_3_indices]  # ìƒìœ„ 3ê°œ í™•ë¥ 
            
            # ìƒìœ„ 3ê°œ í™•ë¥ ì˜ í•©
            top_3_sum = top_3_probs.sum()
            
            # ìƒìœ„ 3ê°œ í™•ë¥ ì„ ì¬ë¶„ë°°: í•©ì´ 0.85ê°€ ë˜ë„ë¡ ìŠ¤ì¼€ì¼ë§
            # (ë‚˜ë¨¸ì§€ 12ê°œ ê°ì •ì´ 0.15ë¥¼ ì°¨ì§€)
            if top_3_sum > 0:
                scale_factor = 0.85 / top_3_sum
                # ìµœëŒ€ 2ë°°ê¹Œì§€ë§Œ ìŠ¤ì¼€ì¼ë§ (ë„ˆë¬´ ê³¼ë„í•˜ê²Œ ì¦ê°€í•˜ì§€ ì•Šë„ë¡)
                scale_factor = min(scale_factor, 2.0)
            else:
                scale_factor = 1.0
            
            # ì „ì²´ í™•ë¥  ë”•ì…”ë„ˆë¦¬ ìƒì„±
            prob_dict = {}
            total_prob = 0.0
            
            for i, prob in enumerate(probabilities):
                label = emotion_labels.get(i, f'í´ë˜ìŠ¤{i}')
                if i in top_3_indices:
                    # ìƒìœ„ 3ê°œëŠ” ìŠ¤ì¼€ì¼ë§ëœ í™•ë¥  ì‚¬ìš©
                    scaled_prob = float(prob * scale_factor)
                    prob_dict[label] = scaled_prob
                    total_prob += scaled_prob
                else:
                    # ë‚˜ë¨¸ì§€ëŠ” ì›ë˜ í™•ë¥ ì„ ì•½ê°„ ì¶•ì†Œ (ìƒìœ„ 3ê°œì— ì§‘ì¤‘)
                    reduced_prob = float(prob * 0.15 / (probabilities.sum() - top_3_sum)) if (probabilities.sum() - top_3_sum) > 0 else float(prob * 0.1)
                    prob_dict[label] = reduced_prob
                    total_prob += reduced_prob
            
            # ìµœì¢… ì •ê·œí™”: ëª¨ë“  í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡
            if total_prob > 0:
                for label in prob_dict:
                    prob_dict[label] = prob_dict[label] / total_prob
            
            # ìµœëŒ€ í™•ë¥ ë„ ì—…ë°ì´íŠ¸
            max_prob_normalized = prob_dict.get(emotion_labels.get(max_prob_idx, 'ì•Œ ìˆ˜ ì—†ìŒ'), max_prob)
            
            return {
                'emotion': final_prediction,
                'emotion_label': final_label,
                'probabilities': prob_dict,
                'confidence': max_prob_normalized,  # ì •ê·œí™”ëœ ìµœëŒ€ í™•ë¥ 
                'original_confidence': max_prob,  # ì›ë˜ ìµœëŒ€ í™•ë¥  (ë””ë²„ê¹…ìš©)
                'original_prediction': int(prediction)  # ì›ë˜ ì˜ˆì¸¡ ê²°ê³¼ë„ í¬í•¨ (ë””ë²„ê¹…ìš©)
            }
            
        except Exception as e:
            ic(f"ML ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            raise
    
    def _predict_dl(self, text: str) -> Dict[str, Any]:
        """DL ëª¨ë¸ ì˜ˆì¸¡"""
        try:
            if not DL_AVAILABLE:
                raise ImportError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if self.dl_model_obj is None or self.dl_model_obj.model is None:
                raise ValueError("DL ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            if self.dl_trainer is None:
                # íŠ¸ë ˆì´ë„ˆ ìƒì„±
                self.dl_trainer = DiaryEmotionDLTrainer(
                    model=self.dl_model_obj.model,
                    tokenizer=self.dl_model_obj.tokenizer,
                    device=self.dl_model_obj.device
                )
            
            # ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
            predictions, probabilities = self.dl_trainer.predict([text], batch_size=1, return_probs=True)
            prediction = predictions[0]
            probabilities = probabilities[0]  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ í™•ë¥ 
            
            # ê°ì • ë¼ë²¨ ë§¤í•‘ (15ê°œ í´ë˜ìŠ¤)
            emotion_labels = {
                0: 'í‰ê°€ë¶ˆê°€', 1: 'ê¸°ì¨', 2: 'ìŠ¬í””', 3: 'ë¶„ë…¸', 4: 'ë‘ë ¤ì›€', 5: 'í˜ì˜¤', 6: 'ë†€ëŒ',
                7: 'ì‹ ë¢°', 8: 'ê¸°ëŒ€', 9: 'ë¶ˆì•ˆ', 10: 'ì•ˆë„', 11: 'í›„íšŒ', 12: 'ê·¸ë¦¬ì›€', 13: 'ê°ì‚¬', 14: 'ì™¸ë¡œì›€'
            }
            
            # ê°€ì¤‘ì¹˜ ì¡°ì • ì „ í™•ë¥  í™•ì¸ (ë””ë²„ê¹…)
            original_max_prob = float(np.max(probabilities))
            original_prediction = int(np.argmax(probabilities))
            ic(f"DL ì›ë³¸ ì˜ˆì¸¡: {emotion_labels.get(original_prediction, 'ì•Œ ìˆ˜ ì—†ìŒ')} (í™•ë¥ : {original_max_prob:.4f})")
            
            # ìƒìœ„ 3ê°œ í™•ë¥  ì¶œë ¥ (ë””ë²„ê¹…)
            top3_indices = np.argsort(probabilities)[-3:][::-1]
            ic("DL ì›ë³¸ ìƒìœ„ 3ê°œ í™•ë¥ :")
            for idx in top3_indices:
                ic(f"  {emotion_labels.get(idx, 'ì•Œ ìˆ˜ ì—†ìŒ')}: {probabilities[idx]:.4f}")
            
            # ê°ì •ë³„ ê°€ì¤‘ì¹˜ ì¡°ì • ì ìš©
            probabilities = self._apply_emotion_weights(probabilities, emotion_labels)
            
            # ìƒìœ„ 3ê°œ ê°ì •ì— í™•ë¥  ì§‘ì¤‘ (Temperature Scaling + Top-3 Boosting)
            probabilities = self._concentrate_top3_probabilities(probabilities, emotion_labels)
            
            # ê°€ì¤‘ì¹˜ ì¡°ì • í›„ ìµœì¢… ì˜ˆì¸¡ (ìµœëŒ€ í™•ë¥ )
            final_prediction = int(np.argmax(probabilities))
            emotion_label = emotion_labels.get(final_prediction, 'ì•Œ ìˆ˜ ì—†ìŒ')
            final_confidence = float(probabilities[final_prediction])
            
            ic(f"DL ìµœì¢… ì˜ˆì¸¡: {emotion_label} (í™•ë¥ : {final_confidence:.4f})")
            
            # í™•ë¥  ë”•ì…”ë„ˆë¦¬ ìƒì„±
            prob_dict = {}
            for idx, label in emotion_labels.items():
                if idx < len(probabilities):
                    prob_dict[label] = float(probabilities[idx])
            
            return {
                'emotion': final_prediction,
                'emotion_label': emotion_label,
                'probabilities': prob_dict,
                'confidence': final_confidence,
                'model_type': 'dl',
                'original_confidence': original_max_prob  # ë””ë²„ê¹…ìš©
            }
            
        except Exception as e:
            ic(f"DL ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            raise
    
    def _concentrate_top3_probabilities(self, probabilities: np.ndarray, emotion_labels: Dict[int, str]) -> np.ndarray:
        """
        ìƒìœ„ 3ê°œ ê°ì •ì— í™•ë¥ ì„ ì§‘ì¤‘ì‹œí‚µë‹ˆë‹¤.
        
        ì „ëµ:
        1. ìƒìœ„ 3ê°œ ê°ì •ì„ ì°¾ìŠµë‹ˆë‹¤
        2. ìƒìœ„ 3ê°œì˜ í™•ë¥ ì€ ì¦í­í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” í¬ê²Œ ê°ì†Œì‹œí‚µë‹ˆë‹¤
        3. ì¬ì •ê·œí™”í•˜ì—¬ í•©ì´ 1ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤
        
        Args:
            probabilities: ê°ì •ë³„ í™•ë¥  ë°°ì—´ (15ê°œ í´ë˜ìŠ¤)
            emotion_labels: ê°ì • ë¼ë²¨ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ìƒìœ„ 3ê°œì— ì§‘ì¤‘ëœ í™•ë¥  ë°°ì—´
        """
        # ìƒìœ„ 3ê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
        top3_indices = np.argsort(probabilities)[-3:][::-1]
        
        # ë””ë²„ê¹…: ìƒìœ„ 3ê°œ í™•ë¥  ì¶œë ¥
        ic("ìƒìœ„ 3ê°œ ê°ì • (ì§‘ì¤‘ ì „):")
        for idx in top3_indices:
            ic(f"  {emotion_labels.get(idx, 'ì•Œ ìˆ˜ ì—†ìŒ')}: {probabilities[idx]:.4f}")
        
        # ìƒˆë¡œìš´ í™•ë¥  ë°°ì—´ ìƒì„±
        concentrated_probs = np.zeros_like(probabilities)
        
        # ìƒìœ„ 3ê°œì˜ í™•ë¥ ì„ ì œê³±í•˜ì—¬ ì¦í­ (Temperature Scaling íš¨ê³¼)
        # ì˜ˆ: 0.1 -> 0.01, 0.2 -> 0.04, 0.3 -> 0.09
        # ê·¸ ë‹¤ìŒ ì •ê·œí™”í•˜ë©´ ë¹„ìœ¨ì´ í¬ê²Œ ë³€í•©ë‹ˆë‹¤
        for idx in top3_indices:
            # í™•ë¥ ì„ ì œê³±í•˜ì—¬ ìƒìœ„ê¶Œê³¼ í•˜ìœ„ê¶Œì˜ ì°¨ì´ë¥¼ ê·¹ëŒ€í™”
            concentrated_probs[idx] = probabilities[idx] ** 0.5  # ì œê³±ê·¼ìœ¼ë¡œ ì•½ê°„ë§Œ ì¦í­ (ë„ˆë¬´ ê·¹ë‹¨ì ì´ì§€ ì•Šê²Œ)
        
        # ë‚˜ë¨¸ì§€ëŠ” ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì • (ì™„ì „íˆ 0ì€ ì•„ë‹˜)
        for idx in range(len(probabilities)):
            if idx not in top3_indices:
                concentrated_probs[idx] = probabilities[idx] * 0.01  # 1%ë§Œ ë‚¨ê¹€
        
        # ì •ê·œí™” (í™•ë¥  í•©ì´ 1ì´ ë˜ë„ë¡)
        concentrated_probs = concentrated_probs / (concentrated_probs.sum() + 1e-10)
        
        # ë””ë²„ê¹…: ìƒìœ„ 3ê°œ í™•ë¥  ì¶œë ¥ (ì§‘ì¤‘ í›„)
        ic("ìƒìœ„ 3ê°œ ê°ì • (ì§‘ì¤‘ í›„):")
        for idx in top3_indices:
            ic(f"  {emotion_labels.get(idx, 'ì•Œ ìˆ˜ ì—†ìŒ')}: {concentrated_probs[idx]:.4f}")
        
        return concentrated_probs
    
    def _apply_keyword_weights(self, text: str, probabilities: np.ndarray, emotion_labels: Dict[int, str]) -> np.ndarray:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë³´ì •"""
        # í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
        text_lower = text.lower()
        
        # ê°ì •ë³„ í‚¤ì›Œë“œ ë° ê°€ì¤‘ì¹˜ ì •ì˜
        keyword_weights = {
            # í‰ê°€ë¶ˆê°€ (ì¤‘ë¦½ì  ë‚´ìš©: ê³µë¬¸ì„œ, ë©”ëª¨, ë‹¨ìˆœ ê¸°ë¡) - ë§¤ìš° ì œí•œì ìœ¼ë¡œë§Œ ì ìš©
            0: {  # í‰ê°€ë¶ˆê°€
                'keywords': [
                    # ê³µë¬¸ì„œ/ê³µë¬´ ê´€ë ¨ (êµ¬ì²´ì ì¸ ê³µì‹ ìš©ì–´ë§Œ)
                    'ê³µë¬¸', 'ê³µë¬´ë¥¼', 'ê³µë¬´ë¥¼ ë´¤ë‹¤', 'ê³µë¬´ë¥¼ ë³´ì•˜ë‹¤', 'ê³µë¬´ë¥¼ ë³¸', 'ê³µë¬´ë¥¼ ë³´ê³ ',
                    'ê³µë¬¸ì„œ', 'ê³µë¬¸ì„', 'ê³µë¬¸ì„ ì¨', 'ê³µë¬¸ì„ ë³´ëƒˆë‹¤', 'ê³µë¬¸ì„ ì‘ì„±',
                    'ë™í—Œì— ë‚˜ê°€', 'ë™í—Œì—ì„œ', 'ë™í—Œì—',
                    # ë¬¸ì„œ/ë³´ê³ ì„œ ê´€ë ¨ (ê³µì‹ì ì¸ ìš©ì–´ë§Œ)
                    'ë¬¸ì„œ ì‘ì„±', 'ë¬¸ì„œ ì‘ì„±í–ˆë‹¤', 'ë¬¸ì„œë¥¼ ì‘ì„±',
                    'ë³´ê³ ì„œ', 'ë³´ê³ ì„œë¥¼', 'ë³´ê³ ë¥¼ ì‘ì„±',
                    'ì‹œí–‰', 'ì‹œë‹¬', 'ê²°ì¬', 'ìŠ¹ì¸', 'ê²°ì¬í–ˆë‹¤', 'ìŠ¹ì¸í–ˆë‹¤',
                    'íšŒì˜ë¡', 'íšŒì˜ë¥¼ ì§„í–‰', 'íšŒì˜ë¥¼ í–ˆë‹¤',
                    'ì•ˆê±´', 'ì•ˆê±´ì„', 'ì•ˆê±´ ì²˜ë¦¬', 'ì•ˆê±´ì„ ì²˜ë¦¬',
                    # ë©”ëª¨/ê¸°ë¡ ê´€ë ¨ (ê³µì‹ì ì¸ ìš©ì–´ë§Œ)
                    'ë©”ëª¨ë¥¼ ì‘ì„±', 'ë©”ëª¨ë¥¼ í–ˆë‹¤',
                    'ê¸°ë¡ì„ ì‘ì„±', 'ê¸°ë¡ì„ í–ˆë‹¤',
                    # ê³µì‹ì /ì—…ë¬´ì  í‘œí˜„
                    'ë¶€ì„', 'ë¶€ì„í–ˆë‹¤', 'ë¶€ì„í•˜ì—¬'
                ],
                'weight': 0.1  # ê°€ì¤‘ì¹˜ ëŒ€í­ ë‚®ì¶¤ (0.3 -> 0.1): í‰ê°€ë¶ˆê°€ íŒì •ì„ ìµœì†Œí™”
            },
            # ê¸ì •ì  ê°ì • (ê¸°ì¨, ê°ì‚¬, ì‹ ë¢°, ê¸°ëŒ€, ì•ˆë„) - ê°€ì¤‘ì¹˜ +1
            1: {  # ê¸°ì¨
                'keywords': [
                    # ê¸°ë³¸ ê¸ì • í‘œí˜„
                    'í–‰ë³µ', 'ì¦ê±°ì›€', 'ê¸°ì¨', 'ì‹ ë‚¨', 'ì„¤ë ˜', 'ì›ƒìŒ', 'ì›ƒì—ˆë‹¤', 'ì›ƒê³ ', 'ì¦ê²', 'ì¬ë¯¸ìˆ', 'ì¬ë°Œ', 
                    'ì¢‹ì•˜', 'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ë§Œì¡±', 'ê¸°ì˜', 'ì‹ ë‚˜', 'ì¦ê±°', 'í–‰ë³µí•˜', 'í–‰ë³µí•œ',
                    'ê¸°ë¶„ ì¢‹', 'ê¸°ë¶„ ì¢‹ì•˜', 'ê¸°ë¶„ ì¢‹ë‹¤', 'ê¸°ë¶„ ì¢‹ì•„', 'ê¸°ë¶„ì´ ì¢‹', 'ê¸°ë¶„ì´ ì¢‹ì•˜', 'ê¸°ë¶„ì´ ì¢‹ë‹¤',
                    'ë§›ìˆ', 'ë§›ìˆì–´', 'ë§›ìˆì—ˆ', 'ë§›ìˆë‹¤', 'ë§›ìˆë„¤', 'ë§›ìˆê³ ',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - ê¸ì • ê°•ì¡° í‘œí˜„
                    'ê°œì¢‹', 'ê°œì©', 'ê°œì¬ë°Œ', 'ê°œì‹ ë‚˜', 'ê°œë§Œì¡±', 'ê°œí–‰ë³µ', 'ê°œì¦ê±°', 'ê°œê¸°ì¨', 'ê°œì›ƒê¹€', 'ê°œì›ƒê²¨',
                    'ì¡´ë‚˜ì¢‹', 'ì¡´ë‚˜ì¢‹ì•„', 'ì¡´ë‚˜ì¢‹ë‹¤', 'ì¡´ë§›', 'ì¡´ë§›íƒ±', 'ì¡´ì¬ë°Œ', 'ì¡´ì‹ ë‚˜', 'ì¡´ë§Œì¡±', 'ì¡´í–‰ë³µ',
                    'ì™„ì „ì¢‹', 'ì™„ì „ì¬ë°Œ', 'ì™„ì „í–‰ë³µ', 'ì™„ì „ë§Œì¡±', 'ì™„ì „ê¸°ì¨', 'ì™„ì „ì¦ê±°',
                    'ì§„ì§œì¢‹', 'ì§„ì§œì¬ë°Œ', 'ì§„ì§œí–‰ë³µ', 'ì§„ì§œë§Œì¡±', 'ì§„ì§œê¸°ì¨',
                    'ë„ˆë¬´ì¢‹', 'ë„ˆë¬´ì¬ë°Œ', 'ë„ˆë¬´í–‰ë³µ', 'ë„ˆë¬´ë§Œì¡±', 'ë„ˆë¬´ê¸°ì¨',
                    'ëŒ€ë°•', 'ëŒ€ë°•ë‚˜', 'ëŒ€ë°•ì´ì•¼', 'ëŒ€ë°•ì´ë‹¤',
                    'ìµœê³ ', 'ìµœê³ ë‹¤', 'ìµœê³ ì•¼', 'ìµœê³ ì„',
                    'ì§±', 'ì§±ì´ì•¼', 'ì§±ì´ë‹¤', 'ì§±ì„',
                    'í—', 'í—ëŒ€ë°•', 'í—ê°œì¢‹', 'í—ì¬ë°Œ'
                ],
                'weight': 1.5  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì•½ê°„ ì¦ê°€
            },
            13: {  # ê°ì‚¬
                'keywords': ['ê°ì‚¬', 'ê³ ë§™', 'ê³ ë§ˆì›Œ', 'ê°ì‚¬í•˜', 'ê°ì‚¬í•œ', 'ê³ ë§ˆ', 'ê³ ë§™ë‹¤', 'ê°ì‚¬í•˜ë‹¤', 'ê³ ë§ˆì›Œìš”', 'ê³ ë§™ìŠµë‹ˆë‹¤'],
                'weight': 1.0
            },
            7: {  # ì‹ ë¢°
                'keywords': ['ë¯¿ìŒ', 'ë¯¿', 'ì‹ ë¢°', 'ë¯¿ì„', 'ë¯¿ê³ ', 'ë¯¿ëŠ”ë‹¤', 'ì‹ ë¢°í•˜', 'ì‹ ë¢°í• '],
                'weight': 1.0
            },
            8: {  # ê¸°ëŒ€
                'keywords': ['ê¸°ëŒ€', 'ê¸°ëŒ€ë˜', 'ê¸°ëŒ€í•œ', 'ê¸°ëŒ€í•˜', 'ê¸°ëŒ€ëœë‹¤', 'ê¸°ëŒ€ë¼', 'ê¸°ëŒ€í•´', 'ê¸°ëŒ€í• '],
                'weight': 1.0
            },
            10: {  # ì•ˆë„
                'keywords': ['ì•ˆì‹¬', 'í¸ì•ˆ', 'ì•ˆë„', 'ì•ˆë„ê°', 'ì•ˆì‹¬ë˜', 'í¸ì•ˆí•˜', 'í¸ì•ˆí•œ', 'ì•ˆì‹¬í•˜', 'ì•ˆë„í•˜', 'ì•ˆì‹¬ëœë‹¤', 'í¸ì•ˆí•˜ë‹¤'],
                'weight': 1.0
            },
            # ë¶€ì •ì  ê°ì • (ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, í˜ì˜¤, ë¶ˆì•ˆ, í›„íšŒ, ì™¸ë¡œì›€) - ê°€ì¤‘ì¹˜ +2
            2: {  # ìŠ¬í””
                'keywords': [
                    # ê¸°ë³¸ ìŠ¬í”” í‘œí˜„
                    'ìŠ¬í”„', 'ìŠ¬í””', 'ëˆˆë¬¼', 'ìš¸ì—ˆ', 'ìš¸ê³ ', 'ìŠ¬í¼', 'ìŠ¬í', 'ìŠ¬í”„ë‹¤', 'ìŠ¬í¼ì„œ', 'ëˆˆë¬¼ì´', 'ëˆˆë¬¼ì„', 'ìš°ìš¸', 'ìš°ìš¸í•˜', 'ìš°ìš¸í•œ', 'ìŠ¬í”„ë„¤', 'ìŠ¬í”„ê³ ',
                    'ì•„ì‰¬', 'ì•„ì‰¬ì›Œ', 'ì•„ì‰¬ì› ', 'ì•„ì‰¬ì› ë‹¤', 'ì•„ì‰½', 'ì•„ì‰½ë‹¤', 'ì•„ì‰¬ì›Œì„œ', 'ì•„ì‰¬ì› ì–´',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - ìŠ¬í”” ê°•ì¡° í‘œí˜„
                    'ê°œìŠ¬í”„', 'ê°œìš°ìš¸', 'ê°œëˆˆë¬¼', 'ê°œìŠ¬í¼',
                    'ì¡´ë‚˜ìŠ¬í”„', 'ì¡´ë‚˜ìš°ìš¸', 'ì¡´ë‚˜ëˆˆë¬¼',
                    'ì™„ì „ìŠ¬í”„', 'ì™„ì „ìš°ìš¸', 'ì™„ì „ëˆˆë¬¼',
                    'ì§„ì§œìŠ¬í”„', 'ì§„ì§œìš°ìš¸', 'ì§„ì§œëˆˆë¬¼'
                ],
                'weight': 2.3  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (2.5 -> 2.3: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”)
            },
            3: {  # ë¶„ë…¸
                'keywords': [
                    # ê¸°ë³¸ ë¶„ë…¸ í‘œí˜„
                    'í™”ë‚˜', 'í™”ë‚¬', 'ì§œì¦', 'ë¶„ë…¸', 'í™”ê°€', 'í™”ë‚¬ë‹¤', 'ì§œì¦ë‚˜', 'ì§œì¦ë‚¬', 'í™”ë‚˜ì„œ', 'ë¶„ë…¸í•˜', 'ë¶„ë…¸í•œ', 'í™”ë‚¬ì–´', 'ì§œì¦ë‚˜ë„¤', 'í™”ë‚˜ë„¤',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - ë¶„ë…¸ ê°•ì¡° í‘œí˜„
                    'ê°œì§œì¦', 'ê°œí™”ë‚˜', 'ê°œë¶„ë…¸', 'ê°œë¹¡', 'ê°œë¹¡ì³', 'ê°œë¹¡ì³¤', 'ê°œë¹¡ì¹¨',
                    'ì¡´ë‚˜ì§œì¦', 'ì¡´ë‚˜í™”ë‚˜', 'ì¡´ë‚˜ë¶„ë…¸', 'ì¡´ë‚˜ë¹¡', 'ì¡´ë‚˜ë¹¡ì³',
                    'ì™„ì „ì§œì¦', 'ì™„ì „í™”ë‚˜', 'ì™„ì „ë¶„ë…¸', 'ì™„ì „ë¹¡',
                    'ì§„ì§œì§œì¦', 'ì§„ì§œí™”ë‚˜', 'ì§„ì§œë¶„ë…¸', 'ì§„ì§œë¹¡',
                    'ë„ˆë¬´ì§œì¦', 'ë„ˆë¬´í™”ë‚˜', 'ë„ˆë¬´ë¶„ë…¸',
                    'í•µì§œì¦', 'í•µë¹¡', 'í•µë¹¡ì¹¨'
                ],
                'weight': 2.3  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (2.5 -> 2.3: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”)
            },
            4: {  # ë‘ë ¤ì›€
                'keywords': [
                    # ê¸°ë³¸ ë‘ë ¤ì›€ í‘œí˜„
                    'ë¬´ì„­', 'ë‘ë µ', 'ë‘ë ¤ì›€', 'ë¬´ì„œì›Œ', 'ë¬´ì„œì› ', 'ë‘ë ¤ì›Œ', 'ë‘ë ¤ì› ', 'ë¬´ì„œ', 'ë‘ë ¤', 'ë¬´ì„­ë‹¤', 'ë‘ë µë‹¤', 'ë¬´ì„œì› ë‹¤', 'ë‘ë ¤ì› ë‹¤', 'ë¬´ì„œì›Œì„œ', 'ë‘ë ¤ì›Œì„œ',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - ë‘ë ¤ì›€ ê°•ì¡° í‘œí˜„
                    'ê°œë¬´ì„œ', 'ê°œë‘ë ¤', 'ê°œë¬´ì„­', 'ê°œë¬´ì„œì›Œ',
                    'ì¡´ë‚˜ë¬´ì„œ', 'ì¡´ë‚˜ë‘ë ¤', 'ì¡´ë‚˜ë¬´ì„­',
                    'ì™„ì „ë¬´ì„œ', 'ì™„ì „ë‘ë ¤', 'ì™„ì „ë¬´ì„­',
                    'ì§„ì§œë¬´ì„œ', 'ì§„ì§œë‘ë ¤', 'ì§„ì§œë¬´ì„­',
                    'ê²ë‚˜ë¬´ì„œ', 'ê²ë‚˜ë‘ë ¤', 'ê²ë‚˜ë¬´ì„­'
                ],
                'weight': 2.3  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (2.5 -> 2.3: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”)
            },
            5: {  # í˜ì˜¤
                'keywords': [
                    # ê¸°ë³¸ í˜ì˜¤ í‘œí˜„
                    'ì‹«', 'í˜ì˜¤', 'ì‹«ì–´', 'ì‹«ë‹¤', 'ì‹«ì—ˆ', 'ì‹«ì€', 'í˜ì˜¤í•˜', 'í˜ì˜¤ìŠ¤ëŸ¬', 'ì‹«ì–´ì„œ', 'ì‹«ì–´ìš”', 'ì‹«ì–´í•´', 'í˜ì˜¤ìŠ¤ëŸ½', 'í˜ì˜¤ìŠ¤ëŸ¬ì›Œ',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - í˜ì˜¤ ê°•ì¡° í‘œí˜„
                    'ê°œì‹«', 'ê°œì—­ê²¹', 'ê°œë”ëŸ¬ì›Œ', 'ê°œë”ëŸ¬ì›€', 'ê°œì§•ê·¸ëŸ¬ì›Œ', 'ê°œì§•ê·¸ëŸ½',
                    'ì¡´ë‚˜ì‹«', 'ì¡´ë‚˜ì—­ê²¹', 'ì¡´ë‚˜ë”ëŸ¬ì›Œ', 'ì¡´ë‚˜ì§•ê·¸ëŸ¬ì›Œ', 'ì¡´ë‚˜ì§•ê·¸ëŸ½',
                    'ì”¹ë…¸ë§›', 'ì”¹ê·¹í˜', 'ì”¹ì—­ê²¹', 'ì”¹ë”ëŸ¬ì›Œ', 'ì”¹ì§•ê·¸ëŸ¬ì›Œ',
                    'ì™„ì „ì‹«', 'ì™„ì „ì—­ê²¹', 'ì™„ì „ë”ëŸ¬ì›Œ', 'ì™„ì „ì§•ê·¸ëŸ¬ì›Œ',
                    'ì§„ì§œì‹«', 'ì§„ì§œì—­ê²¹', 'ì§„ì§œë”ëŸ¬ì›Œ', 'ì§„ì§œì§•ê·¸ëŸ¬ì›Œ',
                    'í•µë¶ˆì¾Œ', 'í•µì—­ê²¹', 'í•µë”ëŸ¬ì›Œ', 'í•µì§•ê·¸ëŸ¬ì›Œ',
                    'ê·¹í˜', 'í† ë‚˜ì™€', 'ìŒ‰', 'ìŒ‰ì‹«'
                ],
                'weight': 2.3  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (2.5 -> 2.3: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”)
            },
            9: {  # ë¶ˆì•ˆ
                'keywords': [
                    # ê¸°ë³¸ ë¶ˆì•ˆ í‘œí˜„
                    'ë¶ˆì•ˆ', 'ê±±ì •', 'ë¶ˆì•ˆí•˜', 'ë¶ˆì•ˆí•œ', 'ê±±ì •ë˜', 'ê±±ì •í•˜', 'ê±±ì •ì´', 'ë¶ˆì•ˆí•´', 'ë¶ˆì•ˆí•˜ë‹¤', 'ê±±ì •ëœë‹¤', 'ê±±ì •ë¼', 'ë¶ˆì•ˆê°', 'ê±±ì •ìŠ¤ëŸ¬',
                    # ë¹„ì†ì–´/ì‹ ì¡°ì–´ - ë¶ˆì•ˆ ê°•ì¡° í‘œí˜„
                    'ê°œë¶ˆì•ˆ', 'ê°œê±±ì •', 'ê°œê±±ì •ë˜', 'ê°œê±±ì •ë¼',
                    'ì¡´ë‚˜ë¶ˆì•ˆ', 'ì¡´ë‚˜ê±±ì •', 'ì¡´ë‚˜ê±±ì •ë˜',
                    'ì™„ì „ë¶ˆì•ˆ', 'ì™„ì „ê±±ì •', 'ì™„ì „ê±±ì •ë˜',
                    'ì§„ì§œë¶ˆì•ˆ', 'ì§„ì§œê±±ì •', 'ì§„ì§œê±±ì •ë˜'
                ],
                'weight': 2.3  # ë¹„ì†ì–´/ì‹ ì¡°ì–´ í¬í•¨ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€ (2.5 -> 2.3: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”)
            },
            11: {  # í›„íšŒ
                'keywords': ['í›„íšŒ', 'í›„íšŒí•˜', 'í›„íšŒí•œ', 'í›„íšŒë˜', 'í›„íšŒë¼', 'í›„íšŒí•´', 'í›„íšŒí•œë‹¤', 'í›„íšŒí•˜ê³ ', 'í›„íšŒí–ˆ', 'í›„íšŒí• '],
                'weight': 1.8  # 2.0 -> 1.8: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”
            },
            14: {  # ì™¸ë¡œì›€
                'keywords': ['ì™¸ë¡­', 'ì™¸ë¡œì›€', 'ì™¸ë¡œì›Œ', 'ì™¸ë¡œì› ', 'ì™¸ë¡­ë‹¤', 'ì™¸ë¡œì›Œì„œ', 'ì™¸ë¡œì› ë‹¤', 'ì™¸ë¡­ë„¤', 'ì™¸ë¡­ê³ ', 'ì™¸ë¡œì›Œìš”'],
                'weight': 1.8  # 2.0 -> 1.8: ë¶€ì • ê°ì • ê³¼ëŒ€í‰ê°€ ì™„í™”
            },
            # ì¤‘ë¦½ì  ê°ì • (ê·¸ë¦¬ì›€, ë†€ëŒ) - ìµœì†Œ ê°€ì¤‘ì¹˜
            12: {  # ê·¸ë¦¬ì›€
                'keywords': ['ê·¸ë¦½', 'ê·¸ë¦¬ì›€', 'ê·¸ë¦¬ì›Œ', 'ê·¸ë¦¬ì› ', 'ê·¸ë¦¬ë‹¤', 'ê·¸ë¦¬ì›Œì„œ', 'ê·¸ë¦¬ì› ë‹¤', 'ë³´ê³ ì‹¶', 'ë³´ê³ ì‹¶ì–´', 'ë³´ê³ ì‹¶ë‹¤', 'ë³´ê³ ì‹¶ì—ˆ'],
                'weight': 0.5
            },
            6: {  # ë†€ëŒ
                'keywords': ['ë†€ë', 'ë†€ëŒ', 'ë†€ë¼', 'ë†€ë', 'ì˜ì™¸', 'ë†€ëë‹¤', 'ë†€ë¼ì›Œ', 'ë†€ë¼ì› ', 'ì˜ì™¸ë‹¤', 'ì˜ì™¸ë„¤', 'ë†€ë¼ì„œ', 'ë†€ëì–´'],
                'weight': 0.5
            }
        }
        
        # ê° ê°ì •ë³„ë¡œ í‚¤ì›Œë“œ ë§¤ì¹­ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
        weight_scores = np.zeros(len(probabilities))
        
        for emotion_id, config in keyword_weights.items():
            if emotion_id >= len(probabilities):
                continue
                
            keywords = config['keywords']
            weight = config['weight']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°
            match_count = sum(1 for keyword in keywords if keyword in text_lower)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© (Word2Vec ì œê±°ë¨)
            if match_count > 0:
                # í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ ê°€ì¤‘ì¹˜ ì ìš© (ë§¤ì¹­ ê°œìˆ˜ì— ë¹„ë¡€)
                weight_scores[emotion_id] = match_count * weight
                ic(f"ê°ì • {emotion_labels.get(emotion_id, emotion_id)}: {match_count}ê°œ í‚¤ì›Œë“œ ë§¤ì¹­, ê°€ì¤‘ì¹˜ {weight_scores[emotion_id]:.3f}")
        
        # ê°€ì¤‘ì¹˜ë¥¼ í™•ë¥ ì— ì ìš© (ì†Œí”„íŠ¸ë§¥ìŠ¤ ë°©ì‹)
        if weight_scores.sum() > 0:
            # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”í•˜ì—¬ í™•ë¥ ì— ë”í•¨
            normalized_weights = weight_scores / (weight_scores.sum() + 1e-10) * 0.25  # ìµœëŒ€ 25% ë³´ì • (15% -> 25%ë¡œ ì¦ê°€)
            adjusted_probs = probabilities + normalized_weights
            
            # í‰ê°€ë¶ˆê°€ í™•ë¥  ì¶”ê°€ ê°ì†Œ: ë‹¤ë¥¸ ê°ì • í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ í‰ê°€ë¶ˆê°€ í™•ë¥ ì„ ë” ë‚®ì¶¤
            if len(probabilities) > 0:
                # í‰ê°€ë¶ˆê°€(0ë²ˆ)ë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ê°ì •ì˜ ê°€ì¤‘ì¹˜ í•© ê³„ì‚°
                other_emotions_weight = weight_scores[1:].sum() if len(weight_scores) > 1 else 0
                
                # ë‹¤ë¥¸ ê°ì • í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì—ˆê³  í‰ê°€ë¶ˆê°€ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í‰ê°€ë¶ˆê°€ í™•ë¥  ê°ì†Œ
                if other_emotions_weight > 0 and weight_scores[0] == 0:
                    # í‰ê°€ë¶ˆê°€ í™•ë¥ ì„ 10% ê°ì†Œ
                    adjusted_probs[0] = adjusted_probs[0] * 0.9
                    ic(f"ë‹¤ë¥¸ ê°ì • í‚¤ì›Œë“œ ë°œê²¬ ({other_emotions_weight:.2f}), í‰ê°€ë¶ˆê°€ í™•ë¥  10% ê°ì†Œ")
                elif other_emotions_weight > weight_scores[0] * 2:
                    # ë‹¤ë¥¸ ê°ì • í‚¤ì›Œë“œê°€ í‰ê°€ë¶ˆê°€ í‚¤ì›Œë“œë³´ë‹¤ 2ë°° ì´ìƒ ë§ìœ¼ë©´ í‰ê°€ë¶ˆê°€ í™•ë¥  10% ê°ì†Œ
                    adjusted_probs[0] = adjusted_probs[0] * 0.9
                    ic(f"ë‹¤ë¥¸ ê°ì • í‚¤ì›Œë“œê°€ ìš°ì„¸ ({other_emotions_weight:.2f} vs {weight_scores[0]:.2f}), í‰ê°€ë¶ˆê°€ í™•ë¥  10% ê°ì†Œ")
            
            # í™•ë¥ ì´ 1ì„ ë„˜ì§€ ì•Šë„ë¡ ì •ê·œí™”
            adjusted_probs = adjusted_probs / (adjusted_probs.sum() + 1e-10)
            
            return adjusted_probs
        
        return probabilities
    
    def _apply_emotion_weights(self, probabilities: np.ndarray, emotion_labels: Dict[int, str]) -> np.ndarray:
        """
        ê°ì •ë³„ ê°€ì¤‘ì¹˜ ì¡°ì • (DL ëª¨ë¸ìš© - ë¯¸ì„¸ ì¡°ì •)
        
        DL ëª¨ë¸ì€ BERT/ELECTRA ê°™ì€ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë¡œ ë¬¸ë§¥ì„ ì˜ ì´í•´í•˜ë¯€ë¡œ,
        ML ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ì‘ì€ ê°€ì¤‘ì¹˜ ì¡°ì •ë§Œ ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            probabilities: ê°ì •ë³„ í™•ë¥  ë°°ì—´ (15ê°œ í´ë˜ìŠ¤)
            emotion_labels: ê°ì • ë¼ë²¨ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ê°€ì¤‘ì¹˜ ì¡°ì •ëœ í™•ë¥  ë°°ì—´
        """
        # DL ëª¨ë¸ì€ ì´ë¯¸ ë¬¸ë§¥ì„ ì˜ ì´í•´í•˜ë¯€ë¡œ í° ì¡°ì • ë¶ˆí•„ìš”
        # í•„ìš”ì‹œ ë¯¸ì„¸ ì¡°ì •ë§Œ ì ìš© (ì˜ˆ: 5-10% ìˆ˜ì¤€)
        
        # í™•ë¥  ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ê°€ì¤‘ì¹˜ ì¡°ì • í¸ì˜ë¥¼ ìœ„í•´)
        prob_dict = {}
        for idx, label in emotion_labels.items():
            if idx < len(probabilities):
                prob_dict[label] = float(probabilities[idx])
        
        # DL ëª¨ë¸ìš© ë¯¸ì„¸ ê°€ì¤‘ì¹˜ ì¡°ì • (MLì˜ 1.2/0.8 ëŒ€ì‹  1.05/0.95 ìˆ˜ì¤€)
        # ë¶ˆì•ˆ: ì•½ê°„ ì¦ê°€ (5% ì¦ê°€) - MLì˜ 20% ì¦ê°€ ëŒ€ë¹„ ë§¤ìš° ì‘ìŒ
        if "ë¶ˆì•ˆ" in prob_dict and prob_dict["ë¶ˆì•ˆ"] > 0.1:  # ë¶ˆì•ˆ í™•ë¥ ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¼ ë•Œë§Œ
            prob_dict["ë¶ˆì•ˆ"] *= 1.05
            ic(f"DL ë¶ˆì•ˆ í™•ë¥  ë¯¸ì„¸ ì¡°ì •: {prob_dict['ë¶ˆì•ˆ']:.4f}")
        
        # ê¸°ëŒ€: ì•½ê°„ ê°ì†Œ (5% ê°ì†Œ) - MLì˜ 20% ê°ì†Œ ëŒ€ë¹„ ë§¤ìš° ì‘ìŒ
        if "ê¸°ëŒ€" in prob_dict and prob_dict["ê¸°ëŒ€"] > 0.1:  # ê¸°ëŒ€ í™•ë¥ ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒì¼ ë•Œë§Œ
            prob_dict["ê¸°ëŒ€"] *= 0.95
            ic(f"DL ê¸°ëŒ€ í™•ë¥  ë¯¸ì„¸ ì¡°ì •: {prob_dict['ê¸°ëŒ€']:.4f}")
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ ë‹¤ì‹œ ë°°ì—´ë¡œ ë³€í™˜
        adjusted_probs = np.array([prob_dict.get(emotion_labels.get(i, ''), 0.0) for i in range(len(probabilities))])
        
        # ì •ê·œí™” ì „ í™•ì¸ (ë””ë²„ê¹…)
        before_norm_max = float(np.max(adjusted_probs))
        before_norm_sum = float(adjusted_probs.sum())
        
        # ì •ê·œí™” (í™•ë¥  í•©ì´ 1ì´ ë˜ë„ë¡)
        adjusted_probs = adjusted_probs / (adjusted_probs.sum() + 1e-10)
        
        # ì •ê·œí™” í›„ í™•ì¸ (ë””ë²„ê¹…)
        after_norm_max = float(np.max(adjusted_probs))
        after_norm_sum = float(adjusted_probs.sum())
        ic(f"DL ê°€ì¤‘ì¹˜ ì¡°ì •: ì •ê·œí™” ì „ ìµœëŒ€={before_norm_max:.4f}, í•©={before_norm_sum:.4f} -> ì •ê·œí™” í›„ ìµœëŒ€={after_norm_max:.4f}, í•©={after_norm_sum:.4f}")
        
        return adjusted_probs
    
    def _try_load_model(self):
        """ëª¨ë¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ (ML ë˜ëŠ” DL)"""
        try:
            # ML ëª¨ë¸ ìë™ ë¡œë“œ
            if self.model_type == "ml" and self.model_file.exists() and self.vectorizer_file.exists():
                ic("ML ëª¨ë¸ íŒŒì¼ ë°œê²¬, ìë™ ë¡œë“œ ì‹œë„...")
                with open(self.model_file, 'rb') as f:
                    self.model_obj.model = pickle.load(f)
                with open(self.vectorizer_file, 'rb') as f:
                    self.model_obj.vectorizer = pickle.load(f)
                # Word2Vec ì œê±°ë¨
                
                # ë©”íƒ€ë°ì´í„° í™•ì¸ (CSV íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸)
                if self.metadata_file.exists():
                    with open(self.metadata_file, 'rb') as f:
                        metadata = pickle.load(f)
                    # pathlibì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸° (os ëŒ€ì‹ )
                    csv_mtime = self.csv_file_path.stat().st_mtime
                    if metadata.get('csv_mtime') == csv_mtime:
                        ic("ML ëª¨ë¸ ìë™ ë¡œë“œ ì„±ê³µ (CSV íŒŒì¼ ë³€ê²½ ì—†ìŒ)")
                        return True
                    else:
                        # CSV íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆì§€ë§Œ, ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš© (ì¬í•™ìŠµ ê¶Œì¥)
                        ic("CSV íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë¨, ê¸°ì¡´ ML ëª¨ë¸ ì‚¬ìš© (ì¬í•™ìŠµ ê¶Œì¥: /train ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ)")
                        # ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        return True
                else:
                    ic("ML ëª¨ë¸ ìë™ ë¡œë“œ ì„±ê³µ (ë©”íƒ€ë°ì´í„° ì—†ìŒ)")
                    return True
            
            # DL ëª¨ë¸ ìë™ ë¡œë“œ
            elif self.model_type == "dl" and self.dl_model_file.exists():
                ic("DL ëª¨ë¸ íŒŒì¼ ë°œê²¬, ìë™ ë¡œë“œ ì‹œë„...")
                return self._load_model_dl()
            
            return False
        except Exception as e:
            ic(f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def save_model(self):
        """ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥ (ML ë˜ëŠ” DL)"""
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°
        if self.model_type == "dl":
            return self._save_model_dl()
        else:
            return self._save_model_ml()
    
    def _save_model_ml(self):
        """ML ëª¨ë¸ ì €ì¥ (ê¸°ì¡´)"""
        try:
            if self.model_obj.model is None or self.model_obj.vectorizer is None:
                raise ValueError("ML ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±)
            try:
                self.model_dir.mkdir(parents=True, exist_ok=True)
                ic(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±: {self.model_dir}")
            except Exception as dir_error:
                ic(f"Path.mkdir ì‹¤íŒ¨: {dir_error}, os.makedirsë¡œ ì¬ì‹œë„...")
                # os.makedirsë¡œ ì¬ì‹œë„ (ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì—ì„œ importë¨)
                os.makedirs(str(self.model_dir), exist_ok=True)
                ic(f"os.makedirsë¡œ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {self.model_dir}")
            
            # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
            if not self.model_dir.exists():
                raise OSError(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_dir}")
            
            # ëª¨ë¸ ì €ì¥
            with open(self.model_file, 'wb') as f:
                pickle.dump(self.model_obj.model, f)
            ic(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_file}")
            
            # Vectorizer ì €ì¥
            with open(self.vectorizer_file, 'wb') as f:
                pickle.dump(self.model_obj.vectorizer, f)
            ic(f"Vectorizer ì €ì¥ ì™„ë£Œ: {self.vectorizer_file}")
            
            # Word2Vec ì œê±°ë¨
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (CSV íŒŒì¼ ìˆ˜ì • ì‹œê°„ í¬í•¨)
            # pathlibì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸° (os ëŒ€ì‹ )
            csv_mtime = self.csv_file_path.stat().st_mtime
            metadata = {
                'csv_mtime': csv_mtime,
                'csv_path': str(self.csv_file_path),
                'trained_at': datetime.now().isoformat(),
                'data_count': len(self.df) if self.df is not None else 0
            }
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            ic(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.metadata_file}")
            
        except Exception as e:
            ic(f"ML ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            raise
    
    def _save_model_dl(self):
        """DL ëª¨ë¸ ì €ì¥"""
        try:
            if not DL_AVAILABLE:
                raise ImportError("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            if self.dl_model_obj is None or self.dl_model_obj.model is None:
                raise ValueError("DL ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. learning()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
            self.model_dir.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ì €ì¥ (PyTorch)
            # ë¡œì»¬ GPUì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ì»¨í…Œì´ë„ˆì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ CPUë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            import torch
            model_state_dict = self.dl_model_obj.model.state_dict()
            
            # GPUì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ CPUë¡œ ë³€í™˜ (ì»¨í…Œì´ë„ˆ í˜¸í™˜ì„±)
            cpu_state_dict = {}
            for key, value in model_state_dict.items():
                cpu_state_dict[key] = value.cpu()
            
            # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì¶”ì¶œ (hidden_size í™•ì¸)
            hidden_size = None
            if hasattr(self.dl_model_obj.model, 'classifier'):
                classifier = self.dl_model_obj.model.classifier
                # Sequentialì¸ ê²½ìš° (2-layer): classifier[0]ì´ Linear
                if isinstance(classifier, torch.nn.Sequential) and len(classifier) > 0:
                    if isinstance(classifier[0], torch.nn.Linear):
                        hidden_size = classifier[0].out_features
                # Linearì¸ ê²½ìš° (1-layer): hidden_sizeëŠ” None
                elif isinstance(classifier, torch.nn.Linear):
                    hidden_size = None
            
            torch.save({
                'model_state_dict': cpu_state_dict,  # CPUë¡œ ë³€í™˜ëœ ìƒíƒœ ì €ì¥
                'model_name': self.dl_model_obj.model_name,
                'num_labels': self.dl_model_obj.num_labels,
                'max_length': self.dl_model_obj.max_length,
                'hidden_size': hidden_size  # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì €ì¥
            }, self.dl_model_file)
            ic(f"DL ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.dl_model_file} (CPU í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥, hidden_size={hidden_size})")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            csv_mtime = self.csv_file_path.stat().st_mtime
            metadata = {
                'model_type': 'dl',
                'model_name': self.dl_model_obj.model_name,
                'num_labels': self.dl_model_obj.num_labels,  # ê°ì • í´ë˜ìŠ¤ ìˆ˜ ì €ì¥
                'max_length': self.dl_model_obj.max_length,
                'hidden_size': hidden_size,  # ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì €ì¥
                'csv_mtime': csv_mtime,
                'csv_path': str(self.csv_file_path),
                'trained_at': datetime.now().isoformat(),
                'data_count': len(self.df) if self.df is not None else 0
            }
            with open(self.dl_metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            ic(f"DL ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.dl_metadata_file}")
            
        except Exception as e:
            ic(f"DL ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            raise
    
    def load_model(self, model_type: Optional[str] = None):
        """ëª¨ë¸ ë¡œë“œ (ML ë˜ëŠ” DL)"""
        target_type = model_type or self.model_type
        
        if target_type == "dl":
            return self._load_model_dl()
        else:
            return self._load_model_ml()
    
    def _load_model_ml(self):
        """ML ëª¨ë¸ ë¡œë“œ"""
        try:
            if not self.model_file.exists():
                ic(f"ML ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.model_file}")
                return False
            
            # ëª¨ë¸ ë¡œë“œ
            with open(self.model_file, 'rb') as f:
                self.model_obj.model = pickle.load(f)
            
            # Vectorizer ë¡œë“œ
            with open(self.vectorizer_file, 'rb') as f:
                self.model_obj.vectorizer = pickle.load(f)
            
            # Word2Vec ì œê±°ë¨
            
            ic("ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            ic(f"ML ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def _load_model_dl(self):
        """DL ëª¨ë¸ ë¡œë“œ"""
        try:
            if not DL_AVAILABLE:
                ic("ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            # torch import (í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ)
            import torch
            
            if not self.dl_model_file.exists():
                ic(f"DL ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.dl_model_file}")
                return False
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(self.dl_metadata_file, 'rb') as f:
                metadata = pickle.load(f)
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            if self.dl_model_obj is None:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ num_labels ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë™ì  ê³„ì‚°)
                num_labels = metadata.get('num_labels', None)
                if num_labels is None and self.df is not None and 'emotion' in self.df.columns:
                    unique_emotions = self.df['emotion'].unique()
                    num_labels = len(unique_emotions)
                elif num_labels is None:
                    num_labels = 15  # ê¸°ë³¸ê°’
                
                self.dl_model_obj = DiaryEmotionDLModel(
                    model_name=metadata['model_name'],
                    num_labels=num_labels,
                    max_length=metadata.get('max_length', 512)
                )
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ hidden_size ê°€ì ¸ì˜¤ê¸° (ëª¨ë¸ êµ¬ì¡° ì¼ì¹˜)
            hidden_size = metadata.get('hidden_size', None)
            # checkpointì—ì„œë„ í™•ì¸ (ë©”íƒ€ë°ì´í„°ì— ì—†ì„ ê²½ìš°)
            if hidden_size is None:
                checkpoint = torch.load(self.dl_model_file, map_location='cpu')
                hidden_size = checkpoint.get('hidden_size', None)
            
            ic(f"ëª¨ë¸ ë¡œë“œ: hidden_size={hidden_size} (Noneì´ë©´ 1-layer, ê°’ì´ ìˆìœ¼ë©´ 2-layer)")
            self.dl_model_obj.create_model(dropout_rate=0.3, hidden_size=hidden_size)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            checkpoint = torch.load(self.dl_model_file, map_location=self.dl_model_obj.device)
            self.dl_model_obj.model.load_state_dict(checkpoint['model_state_dict'])
            self.dl_model_obj.model.eval()
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            self.dl_trainer = DiaryEmotionDLTrainer(
                model=self.dl_model_obj.model,
                tokenizer=self.dl_model_obj.tokenizer,
                device=self.dl_model_obj.device
            )
            
            ic("DL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            ic(f"DL ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def submit(self):
        """ì œì¶œ/ëª¨ë¸ ì €ì¥"""
        ic("ğŸ˜ğŸ˜ ì œì¶œ ì‹œì‘")
        self.save_model()
        ic("ğŸ˜ğŸ˜ ì œì¶œ ì™„ë£Œ")

