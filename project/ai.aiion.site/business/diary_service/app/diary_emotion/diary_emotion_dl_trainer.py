"""
Diary Emotion DL Trainer
ì¼ê¸° ê°ì • ë¶„ë¥˜ ë”¥ëŸ¬ë‹ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
"""

from typing import Optional, Tuple, Dict, Any, List
import pandas as pd
import numpy as np
from icecream import ic
from pathlib import Path

# PyTorch ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    from torch.utils.data import Dataset, DataLoader
    from torch import nn
    from torch.optim import AdamW
    from transformers import get_linear_schedule_with_warmup
    from tqdm import tqdm
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    ic("ê²½ê³ : torch ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class EmotionDataset(Dataset):
    """ê°ì • ë¶„ë¥˜ ë°ì´í„°ì…‹ (PyTorch)"""
    
    def __init__(
        self,
        texts: List[str],
        labels: List[int],
        tokenizer,
        max_length: int = 512
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            tokenizer: HuggingFace í† í¬ë‚˜ì´ì €
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class DiaryEmotionDLTrainer:
    """ì¼ê¸° ê°ì • ë¶„ë¥˜ ë”¥ëŸ¬ë‹ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(
        self,
        model,
        tokenizer,
        device: Optional[torch.device] = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model: BERTEmotionClassifier ëª¨ë¸
            tokenizer: HuggingFace í† í¬ë‚˜ì´ì €
            device: ë””ë°”ì´ìŠ¤ 
                    - None (ê¸°ë³¸ê°’): GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©, ì—†ìœ¼ë©´ CPU
                    - torch.device("cuda"): GPU ëª…ì‹œì  ìš”ì²­ (ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜)
                    - torch.device("cpu"): CPU ëª…ì‹œì  ìš”ì²­ (GPUê°€ ìˆì–´ë„ CPU ì‚¬ìš©)
                    
        Note:
            GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ device íŒŒë¼ë¯¸í„°ì™€ ê´€ê³„ì—†ì´ GPUë¥¼ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
            CPUë¥¼ ê°•ì œë¡œ ì‚¬ìš©í•˜ë ¤ë©´ device=torch.device("cpu")ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
        """
        if not TORCH_AVAILABLE:
            raise ImportError("torch ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.model = model
        self.tokenizer = tokenizer
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° device ì„¤ì •
        # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ í•­ìƒ GPU ì‚¬ìš© (device íŒŒë¼ë¯¸í„° ë¬´ì‹œ)
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            ic(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            ic(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            if device is not None:
                if device.type == "cpu":
                    ic("âš ï¸ ì „ë‹¬ëœ deviceê°€ CPUì´ì§€ë§Œ, GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ GPUë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                elif device.type == "cuda":
                    ic("âœ… GPU ì‚¬ìš© (ëª…ì‹œì  ìš”ì²­)")
        else:
            # GPUê°€ ì—†ì„ ë•Œ
            if device is not None and device.type == "cuda":
                raise RuntimeError("CUDA deviceë¥¼ ìš”ì²­í–ˆì§€ë§Œ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CUDAê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            self.device = torch.device("cpu")
            ic("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
            if device is not None:
                ic(f"   ì „ë‹¬ëœ device: {device}")
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model.to(self.device)
        
        # GPU ì‚¬ìš© í™•ì¸
        if self.device.type == "cuda":
            ic(f"âœ… ëª¨ë¸ì´ GPUì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {next(self.model.parameters()).device}")
        else:
            ic(f"âš ï¸ ëª¨ë¸ì´ CPUì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {next(self.model.parameters()).device}")
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
        ic(f"DiaryEmotionDLTrainer ì´ˆê¸°í™” ì™„ë£Œ: device={self.device}")
    
    def create_dataloader(
        self,
        texts: List[str],
        labels: List[int],
        batch_size: int = 16,
        max_length: int = 512,
        shuffle: bool = True
    ) -> DataLoader:
        """
        DataLoader ìƒì„±
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            shuffle: ì„ê¸° ì—¬ë¶€
        
        Returns:
            DataLoader
        """
        dataset = EmotionDataset(
            texts=texts,
            labels=labels,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        # num_workers ì„¤ì • (ë°ì´í„° ë¡œë”© ë³‘ëª© í•´ì†Œ)
        # Docker ì»¨í…Œì´ë„ˆì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´
        import platform
        import os
        
        # í™˜ê²½ ë³€ìˆ˜ë¡œ num_workers ì œì–´ (ê¸°ë³¸ê°’: ìë™ ê°ì§€)
        env_num_workers = os.getenv('DATALOADER_NUM_WORKERS')
        if env_num_workers is not None:
            num_workers = int(env_num_workers)
            ic(f"DataLoader num_workers ì„¤ì •: {num_workers} (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •)")
        else:
            # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì¸ì§€ í™•ì¸
            is_docker = os.path.exists('/.dockerenv')
            is_linux = platform.system() == "Linux"
            
            # Docker ì»¨í…Œì´ë„ˆì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
            if is_docker:
                num_workers = 0  # Docker ì»¨í…Œì´ë„ˆì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
                ic(f"DataLoader num_workers ì„¤ì •: {num_workers} (Docker ì»¨í…Œì´ë„ˆ í™˜ê²½)")
            elif is_linux and platform.system() != "Windows":
                # ë¡œì»¬ Linux í™˜ê²½ì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
                try:
                    import multiprocessing
                    num_workers = min(multiprocessing.cpu_count(), 4)  # ìµœëŒ€ 4ê°œë¡œ ì œí•œ
                except:
                    num_workers = 2
                ic(f"DataLoader num_workers ì„¤ì •: {num_workers} (Linux í™˜ê²½, ë©€í‹°í”„ë¡œì„¸ì‹± í™œì„±í™”)")
            else:
                num_workers = 0  # Windowsì—ì„œëŠ” 0 (ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ)
                ic(f"DataLoader num_workers ì„¤ì •: {num_workers} (Windows í™˜ê²½)")
        
        # GPU ì‚¬ìš© ì‹œ pin_memory í™œì„±í™” (ë°ì´í„° ì „ì†¡ ì†ë„ í–¥ìƒ)
        pin_memory = self.device.type == "cuda"
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=True if num_workers > 0 else False  # ì›Œì»¤ ì¬ì‚¬ìš©ìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
        )
    
    def train_epoch(
        self,
        train_loader: DataLoader,
        optimizer,
        scheduler,
        criterion,
        use_amp: bool = True  # Mixed Precision Training (FP16)
    ) -> Tuple[float, float]:
        """
        í•œ ì—í­ í•™ìŠµ
        
        Args:
            train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
            optimizer: ì˜µí‹°ë§ˆì´ì €
            scheduler: ìŠ¤ì¼€ì¤„ëŸ¬
            criterion: ì†ì‹¤ í•¨ìˆ˜
            use_amp: Mixed Precision Training ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
        
        Returns:
            (í‰ê·  ì†ì‹¤, ì •í™•ë„)
        """
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        # Mixed Precision Training ì„¤ì • (FP16)
        scaler = None
        if use_amp and self.device.type == "cuda":
            try:
                # ìƒˆë¡œìš´ API ì‚¬ìš© (PyTorch 2.0+)
                scaler = torch.amp.GradScaler('cuda')
                ic("âœ… Mixed Precision Training (FP16) í™œì„±í™”")
            except (AttributeError, TypeError):
                # í•˜ìœ„ í˜¸í™˜ì„±: êµ¬ë²„ì „ PyTorch ì§€ì›
                try:
                    scaler = torch.cuda.amp.GradScaler()
                    ic("âœ… Mixed Precision Training (FP16) í™œì„±í™” (êµ¬ë²„ì „ API)")
                except AttributeError:
                    ic("âš ï¸ Mixed Precision Trainingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    use_amp = False
        
        progress_bar = tqdm(train_loader, desc="Training")
        
        for batch_idx, batch in enumerate(progress_bar):
            # ë°ì´í„° ì´ë™ (GPU ì‚¬ìš© ì‹œ ë¹„ë™ê¸° ì „ì†¡)
            input_ids = batch['input_ids'].to(self.device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
            labels = batch['labels'].to(self.device, non_blocking=True)
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ë””ë°”ì´ìŠ¤ í™•ì¸
            if batch_idx == 0:
                ic(f"ì²« ë²ˆì§¸ ë°°ì¹˜ ë””ë°”ì´ìŠ¤ í™•ì¸: input_ids.device={input_ids.device}")
                if use_amp and scaler:
                    ic(f"Mixed Precision Training í™œì„±í™”ë¨")
            
            # Mixed Precision Training ì‚¬ìš©
            if use_amp and scaler:
                # FP16ìœ¼ë¡œ ìˆœì „íŒŒ (ìƒˆë¡œìš´ API ì‚¬ìš©)
                try:
                    # PyTorch 2.0+ ìƒˆë¡œìš´ API
                    autocast_context = torch.amp.autocast('cuda')
                except (AttributeError, TypeError):
                    # í•˜ìœ„ í˜¸í™˜ì„±: êµ¬ë²„ì „ PyTorch
                    autocast_context = torch.cuda.amp.autocast()
                
                with autocast_context:
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )
                    loss = criterion(outputs, labels)
                
                # ì—­ì „íŒŒ (FP16)
                optimizer.zero_grad()
                scaler.scale(loss).backward()
                
                # Gradient clipping (FP16)
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í… (FP16)
                scaler.step(optimizer)
                scaler.update()
            else:
                # ì¼ë°˜ ëª¨ë“œ (FP32)
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                loss = criterion(outputs, labels)
                
                # ì—­ì „íŒŒ
                optimizer.zero_grad()
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
            
            scheduler.step()
            
            # í†µê³„
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'loss': loss.item(),
                'acc': 100 * correct / total
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def evaluate(
        self,
        val_loader: DataLoader,
        criterion
    ) -> Tuple[float, float, np.ndarray, np.ndarray]:
        """
        í‰ê°€
        
        Args:
            val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
            criterion: ì†ì‹¤ í•¨ìˆ˜
        
        Returns:
            (í‰ê·  ì†ì‹¤, ì •í™•ë„, ì‹¤ì œ ë¼ë²¨, ì˜ˆì¸¡ ë¼ë²¨)
        """
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        all_labels = []
        all_predictions = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Evaluating"):
                # ë°ì´í„° ì´ë™
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # ìˆœì „íŒŒ
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                # ì†ì‹¤ ê³„ì‚°
                loss = criterion(outputs, labels)
                
                # í†µê³„
                total_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # ë¼ë²¨ ì €ì¥
                all_labels.extend(labels.cpu().numpy())
                all_predictions.extend(predicted.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy, np.array(all_labels), np.array(all_predictions)
    
    def train(
        self,
        train_texts: List[str],
        train_labels: List[int],
        val_texts: List[str],
        val_labels: List[int],
        epochs: int = 3,
        batch_size: int = 8,
        learning_rate: float = 2e-5,
        max_length: int = 256,
        freeze_bert_layers: int = 0,
        early_stopping_patience: int = 3,
        use_amp: bool = True,  # Mixed Precision Training (FP16) - RTX 4060 ìµœì í™”
        label_smoothing: float = 0.0  # Label smoothing (0.0 = ë¹„í™œì„±í™”, 0.1 = ê¶Œì¥ê°’)
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            train_texts: í•™ìŠµ í…ìŠ¤íŠ¸
            train_labels: í•™ìŠµ ë¼ë²¨
            val_texts: ê²€ì¦ í…ìŠ¤íŠ¸
            val_labels: ê²€ì¦ ë¼ë²¨
            epochs: ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            freeze_bert_layers: ë™ê²°í•  BERT ë ˆì´ì–´ ìˆ˜
            early_stopping_patience: Early stopping patience
            use_amp: Mixed Precision Training ì‚¬ìš© ì—¬ë¶€
            label_smoothing: Label smoothing ê°’ (0.0 = ë¹„í™œì„±í™”, 0.1 = ê¶Œì¥ê°’, ê³¼ì í•© ë°©ì§€)
        
        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        ic(f"í•™ìŠµ ì‹œì‘: epochs={epochs}, batch_size={batch_size}, lr={learning_rate}")
        if use_amp and self.device.type == "cuda":
            ic("âœ… Mixed Precision Training (FP16) í™œì„±í™” - í•™ìŠµ ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ í–¥ìƒ")
        
        # BERT ë ˆì´ì–´ ë™ê²°
        if freeze_bert_layers > 0:
            self.model.freeze_bert_layers(freeze_bert_layers)
        
        # DataLoader ìƒì„±
        train_loader = self.create_dataloader(
            train_texts, train_labels, batch_size, max_length, shuffle=True
        )
        val_loader = self.create_dataloader(
            val_texts, val_labels, batch_size, max_length, shuffle=False
        )
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        optimizer = AdamW(
            self.model.parameters(),
            lr=learning_rate,
            eps=1e-8
        )
        
        total_steps = len(train_loader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),
            num_training_steps=total_steps
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ + Label Smoothing)
        # ë¼ë²¨ ë¶„í¬ ê³„ì‚°
        unique, counts = np.unique(train_labels, return_counts=True)
        class_weights = 1.0 / counts
        class_weights = class_weights / class_weights.sum() * len(unique)
        class_weights = torch.FloatTensor(class_weights).to(self.device)
        
        # Label Smoothing ì ìš© (ê³¼ì í•© ë°©ì§€)
        if label_smoothing > 0.0:
            ic(f"âœ… Label Smoothing í™œì„±í™”: {label_smoothing} (ê³¼ì í•© ë°©ì§€)")
            criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)
        else:
            criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        # í•™ìŠµ ë£¨í”„
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            ic(f"Epoch {epoch+1}/{epochs}")
            
            # í•™ìŠµ (Mixed Precision Training ì‚¬ìš©)
            train_loss, train_acc = self.train_epoch(
                train_loader, optimizer, scheduler, criterion, use_amp=use_amp
            )
            
            # í‰ê°€
            val_loss, val_acc, val_labels, val_preds = self.evaluate(
                val_loader, criterion
            )
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            ic(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            ic(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            
            # Early stopping
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                ic(f"âœ… Best model updated: Val Acc={best_val_acc:.4f}")
            else:
                patience_counter += 1
                ic(f"âš ï¸ Patience: {patience_counter}/{early_stopping_patience}")
                
                if patience_counter >= early_stopping_patience:
                    ic(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                    break
        
        return {
            "train_losses": self.train_losses,
            "val_losses": self.val_losses,
            "train_accuracies": self.train_accuracies,
            "val_accuracies": self.val_accuracies,
            "best_val_accuracy": best_val_acc,
            "final_train_accuracy": self.train_accuracies[-1] if self.train_accuracies else 0.0,
            "final_val_accuracy": self.val_accuracies[-1] if self.val_accuracies else 0.0,
            "final_train_loss": self.train_losses[-1] if self.train_losses else 0.0,
            "final_val_loss": self.val_losses[-1] if self.val_losses else 0.0
        }
    
    def predict(
        self,
        texts: List[str],
        batch_size: int = 16,
        max_length: int = 512,
        return_probs: bool = False
    ):
        """
        ì˜ˆì¸¡
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            return_probs: Trueë©´ í™•ë¥ ë„ ë°˜í™˜
        
        Returns:
            return_probs=False: ì˜ˆì¸¡ ë¼ë²¨ ë°°ì—´
            return_probs=True: (ì˜ˆì¸¡ ë¼ë²¨ ë°°ì—´, í™•ë¥  ë°°ì—´)
        """
        self.model.eval()
        
        # ì„ì‹œ ë¼ë²¨ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        dummy_labels = [0] * len(texts)
        
        # DataLoader ìƒì„±
        dataloader = self.create_dataloader(
            texts, dummy_labels, batch_size, max_length, shuffle=False
        )
        
        all_predictions = []
        all_probabilities = []
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Predicting"):
                # ë°ì´í„° ì´ë™
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # ìˆœì „íŒŒ
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                # í™•ë¥  ê³„ì‚° (softmax)
                probs = torch.softmax(outputs, dim=1)
                
                # ì˜ˆì¸¡
                _, predicted = torch.max(outputs, 1)
                all_predictions.extend(predicted.cpu().numpy())
                
                if return_probs:
                    all_probabilities.extend(probs.cpu().numpy())
        
        if return_probs:
            return np.array(all_predictions), np.array(all_probabilities)
        else:
            return np.array(all_predictions)

