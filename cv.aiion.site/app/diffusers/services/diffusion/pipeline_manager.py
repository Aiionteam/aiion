import torch
from diffusers import (
    AutoPipelineForText2Image,
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
    UNet2DConditionModel,
    AutoencoderKL,
    DPMSolverMultistepScheduler,
    EulerDiscreteScheduler,
)
from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer
from safetensors import safe_open
from app.diffusers.core.config import MODEL_ID, DEVICE, DTYPE, HF_CACHE_DIR, SCHEDULER_TYPE, USE_KARRAS, USE_REFINER, DEFAULT_REFINER_STRENGTH

_PIPELINE = None
_PIPELINE_LOADED = False  # íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì—¬ë¶€ ì¶”ì 
_REFINER_PIPELINE = None
_REFINER_LOADED = False  # Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì—¬ë¶€ ì¶”ì 

def _torch_dtype():
    """ì„¤ì •ì— ë”°ë¥¸ torch dtype ë°˜í™˜"""
    if DTYPE.lower() == "float16":
        return torch.float16
    if DTYPE.lower() == "bfloat16":
        return torch.bfloat16
    return torch.float32

def _load_from_single_files(model_dir):
    """
    ë‹¨ì¼ safetensors íŒŒì¼ì—ì„œ SDXL íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    - sd_xl_base_1.0.safetensors: UNet
    - sdxl.vae.safetensors: VAE
    - text_encoder, text_encoder_2: Hugging Faceì—ì„œ ë¡œë“œ
    """
    from pathlib import Path
    model_path = Path(model_dir)
    dtype = _torch_dtype()
    
    print("ğŸ“¦ ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ ì¤‘...")
    
    # 1. Text Encoders (Hugging Faceì—ì„œ ë¡œë“œ)
    print("  [1/4] Text Encoders ë¡œë“œ ì¤‘...")
    text_encoder = CLIPTextModel.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        subfolder="text_encoder",
        dtype=dtype,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©
        cache_dir=str(HF_CACHE_DIR),
    )
    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        subfolder="text_encoder_2",
        dtype=dtype,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©
        cache_dir=str(HF_CACHE_DIR),
    )
    tokenizer = CLIPTokenizer.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        subfolder="tokenizer",
        cache_dir=str(HF_CACHE_DIR),
    )
    tokenizer_2 = CLIPTokenizer.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        subfolder="tokenizer_2",
        cache_dir=str(HF_CACHE_DIR),
    )
    print("  âœ… Text Encoders ë¡œë“œ ì™„ë£Œ")
    
    # 2. UNet (ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ë¡œë“œ)
    print("  [2/4] UNet ë¡œë“œ ì¤‘...")
    unet_path = model_path / "sd_xl_base_1.0.safetensors"
    if not unet_path.exists():
        raise FileNotFoundError(f"UNet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {unet_path}")
    
    # UNet configëŠ” Hugging Faceì—ì„œ ê°€ì ¸ì˜¤ê¸°
    unet = UNet2DConditionModel.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        subfolder="unet",
        dtype=dtype,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©
        cache_dir=str(HF_CACHE_DIR),
    )
    
    # ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # sd_xl_base_1.0.safetensorsëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ
    # UNet ëª¨ë¸ì˜ í‚¤ êµ¬ì¡° í™•ì¸
    unet_model_keys = set(unet.state_dict().keys())
    
    # ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë“  í‚¤ ë¡œë“œ
    file_state_dict = {}
    with safe_open(str(unet_path), framework="pt", device="cpu") as f:
        for key in f.keys():
            file_state_dict[key] = f.get_tensor(key)
    
    # UNet ëª¨ë¸ í‚¤ì™€ ë§¤ì¹­
    unet_state_dict = {}
    matched_count = 0
    
    # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
    for model_key in unet_model_keys:
        if model_key in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[model_key]
            matched_count += 1
        # 2. ComfyUI í˜•ì‹ ë³€í™˜ ì‹œë„ (model.diffusion_model. -> )
        elif f"model.diffusion_model.{model_key}" in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[f"model.diffusion_model.{model_key}"]
            matched_count += 1
        # 3. diffusion_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"diffusion_model.{model_key}" in file_state_dict:
            unet_state_dict[model_key] = file_state_dict[f"diffusion_model.{model_key}"]
            matched_count += 1
    
    print(f"  ğŸ“Š í‚¤ ë§¤ì¹­: {matched_count}/{len(unet_model_keys)}ê°œ")
    
    # ë¡œë“œëœ í‚¤ í™•ì¸
    missing_keys, unexpected_keys = unet.load_state_dict(unet_state_dict, strict=False)
    if len(missing_keys) > 100:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
        print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
    elif missing_keys:
        print(f"  âš ï¸  UNet ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
    if unexpected_keys:
        print(f"  âš ï¸  UNet ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
    print(f"  âœ… UNet ë¡œë“œ ì™„ë£Œ")
    
    # 3. VAE (ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ë¡œë“œ)
    print("  [3/4] VAE ë¡œë“œ ì¤‘...")
    vae_path = model_path / "sdxl.vae.safetensors"
    if not vae_path.exists():
        raise FileNotFoundError(f"VAE íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vae_path}")
    
    # VAE configëŠ” ë¡œì»¬ config.json ì‚¬ìš© (ìˆìœ¼ë©´)
    vae_config_path = model_path / "config.json"
    if vae_config_path.exists():
        # ë¡œì»¬ config ì‚¬ìš©
        import json
        vae_config = json.loads(vae_config_path.read_text())
        # _class_name, _diffusers_version ë“± ì œê±°
        vae_config_clean = {k: v for k, v in vae_config.items() if not k.startswith('_')}
        vae = AutoencoderKL(**vae_config_clean)
    else:
        # Hugging Faceì—ì„œ config ê°€ì ¸ì˜¤ê¸°
        vae = AutoencoderKL.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            subfolder="vae",
            dtype=dtype,  # torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©
            cache_dir=str(HF_CACHE_DIR),
        )
    
    # ë¡œì»¬ safetensors íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # VAE ëª¨ë¸ì˜ í‚¤ êµ¬ì¡° í™•ì¸
    vae_model_keys = set(vae.state_dict().keys())
    
    # ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë“  í‚¤ ë¡œë“œ
    file_state_dict = {}
    with safe_open(str(vae_path), framework="pt", device="cpu") as f:
        for key in f.keys():
            file_state_dict[key] = f.get_tensor(key)
    
    # VAE ëª¨ë¸ í‚¤ì™€ ë§¤ì¹­
    vae_state_dict = {}
    matched_count = 0
    
    # 1. ì§ì ‘ ë§¤ì¹­ ì‹œë„
    for model_key in vae_model_keys:
        if model_key in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[model_key]
            matched_count += 1
        # 2. first_stage_model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"first_stage_model.{model_key}" in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[f"first_stage_model.{model_key}"]
            matched_count += 1
        # 3. model. ì ‘ë‘ì‚¬ ì œê±° ì‹œë„
        elif f"model.{model_key}" in file_state_dict:
            vae_state_dict[model_key] = file_state_dict[f"model.{model_key}"]
            matched_count += 1
    
    print(f"  ğŸ“Š í‚¤ ë§¤ì¹­: {matched_count}/{len(vae_model_keys)}ê°œ")
    
    # ë¡œë“œëœ í‚¤ í™•ì¸
    missing_keys, unexpected_keys = vae.load_state_dict(vae_state_dict, strict=False)
    if len(missing_keys) > 50:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        print(f"  âš ï¸  VAE ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ (ì¼ë¶€ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
        print(f"      ìƒ˜í”Œ: {list(missing_keys)[:5]}")
    elif missing_keys:
        print(f"  âš ï¸  VAE ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
    if unexpected_keys:
        print(f"  âš ï¸  VAE ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
    # VAE dtype ì„¤ì • (ê²½ê³  ë°©ì§€)
    # VAEëŠ” ë””ì½”ë”© ì‹œ float32ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    vae = vae.to(dtype=dtype)
    # upcast_vae deprecation ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
    if hasattr(vae, 'enable_slicing'):
        vae.enable_slicing()
    if hasattr(vae, 'enable_tiling'):
        vae.enable_tiling()
    print(f"  âœ… VAE ë¡œë“œ ì™„ë£Œ")
    
    # 4. Scheduler (Hugging Faceì—ì„œ ë¡œë“œ)
    print("  [4/4] Scheduler ë¡œë“œ ì¤‘...")
    if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
        # DPM++ 2M Karras (ê³ í’ˆì§ˆ ì¡°í•©)
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            subfolder="scheduler",
            cache_dir=str(HF_CACHE_DIR),
        )
        # Karras ì‹œê·¸ë§ˆ ìŠ¤ì¼€ì¤„ ì ìš©
        scheduler = DPMSolverMultistepScheduler.from_config(
            scheduler.config,
            use_karras=True,
        )
        print("  âœ… DPM++ Multistep Scheduler (Karras) ë¡œë“œ ì™„ë£Œ")
    else:
        # Euler (ê¸°ë³¸)
        scheduler = EulerDiscreteScheduler.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            subfolder="scheduler",
            cache_dir=str(HF_CACHE_DIR),
        )
        print("  âœ… Euler Discrete Scheduler ë¡œë“œ ì™„ë£Œ")
    
    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    print("ğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
    pipe = StableDiffusionXLPipeline(
        vae=vae,
        text_encoder=text_encoder,
        text_encoder_2=text_encoder_2,
        tokenizer=tokenizer,
        tokenizer_2=tokenizer_2,
        unet=unet,
        scheduler=scheduler,
    )
    
    return pipe

def get_pipeline():
    """
    SDXL íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤ ë¡œë“œ ë° ìµœì í™”
    RTX 4060 8GB í™˜ê²½ì— ìµœì í™”ë¨
    ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ ì§€ì›
    """
    global _PIPELINE
    if _PIPELINE is not None:
        return _PIPELINE

    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_ID}")
    dtype = _torch_dtype()

    # ë¡œì»¬ ëª¨ë¸ì¸ì§€ Hugging Face ëª¨ë¸ì¸ì§€ í™•ì¸
    from pathlib import Path
    model_path = Path(MODEL_ID)
    is_local = model_path.exists() and (model_path / "model_index.json").exists()
    
    if is_local:
        print(f"ğŸ“ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©: {MODEL_ID}")
        
        # í‘œì¤€ diffusers í˜•ì‹ í™•ì¸ (ìš°ì„ )
        has_text_encoder = (model_path / "text_encoder").exists()
        has_unet = (model_path / "unet").exists()
        has_vae = (model_path / "vae").exists()
        
        # ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ í™•ì¸ (ëŒ€ì²´)
        has_unet_file = (model_path / "sd_xl_base_1.0.safetensors").exists()
        has_vae_file = (model_path / "sdxl.vae.safetensors").exists()
        
        if has_text_encoder and has_unet and has_vae:
            # í‘œì¤€ diffusers í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (ì™„ì „ ë¡œì»¬)
            print("  ğŸ“¦ í‘œì¤€ diffusers í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (ì™„ì „ ë¡œì»¬)")
            try:
                # VAE: sdxl.vae.safetensors ìš°ì„  ì‚¬ìš© (ìƒ‰ê° ë³´ì¡´)
                # ë‹¨ì¼ íŒŒì¼ í˜•ì‹ì˜ VAEê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                vae_single_file = model_path / "sdxl.vae.safetensors"
                if vae_single_file.exists():
                    print("  ğŸ¨ sdxl.vae.safetensors ì‚¬ìš© (ìƒ‰ê° ë³´ì¡´)")
                    # VAEë¥¼ ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¡œë“œ
                    from diffusers import AutoencoderKL
                    vae = AutoencoderKL.from_pretrained(
                        MODEL_ID,
                        subfolder="vae",
                        torch_dtype=dtype,
                        local_files_only=True,
                    )
                    # sdxl.vae.safetensorsì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
                    from safetensors import safe_open
                    vae_state_dict = {}
                    with safe_open(str(vae_single_file), framework="pt", device="cpu") as f:
                        for key in f.keys():
                            if key.startswith("first_stage_model."):
                                vae_state_dict[key.replace("first_stage_model.", "")] = f.get_tensor(key)
                            elif key.startswith("vae."):
                                vae_state_dict[key[4:]] = f.get_tensor(key)
                            elif key.startswith("model."):
                                vae_state_dict[key[6:]] = f.get_tensor(key)
                            else:
                                vae_state_dict[key] = f.get_tensor(key)
                    vae.load_state_dict(vae_state_dict, strict=False)
                    vae = vae.to(dtype=dtype)
                    
                    # ë‚˜ë¨¸ì§€ ì»´í¬ë„ŒíŠ¸ëŠ” í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ
                    pipe = AutoPipelineForText2Image.from_pretrained(
                        MODEL_ID,
                        torch_dtype=dtype,
                        variant=None,
                        use_safetensors=True,
                        local_files_only=True,
                    )
                    # VAE êµì²´
                    pipe.vae = vae
                else:
                    # í‘œì¤€ í˜•ì‹ VAE ì‚¬ìš©
                    pipe = AutoPipelineForText2Image.from_pretrained(
                        MODEL_ID,
                        torch_dtype=dtype,  # dtype ë³€ìˆ˜ ì‚¬ìš© (float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
                        variant=None,
                        use_safetensors=True,
                        local_files_only=True,  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                    )
                
                # torch_dtypeìœ¼ë¡œ ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”
                # (from_pretrainedì˜ torch_dtype íŒŒë¼ë¯¸í„°ê°€ ì´ë¯¸ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì— ì ìš©ë¨)
                
                # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© (í‘œì¤€ í˜•ì‹)
                if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
                    print("  ğŸ”¥ Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì¤‘...")
                    pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                        pipe.scheduler.config,
                        use_karras=True,
                    )
                    print("  âœ… DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ í‘œì¤€ í˜•ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("  ğŸ’¡ download_model_local.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                raise
        elif has_unet_file and has_vae_file:
            # ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (Text EncodersëŠ” Hugging Faceì—ì„œ)
            print("  ğŸ“¦ ë‹¨ì¼ safetensors íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë¡œë“œ (Text EncodersëŠ” Hugging Faceì—ì„œ)")
            try:
                pipe = _load_from_single_files(model_path)
            except Exception as e:
                print(f"  âŒ ë‹¨ì¼ íŒŒì¼ í˜•ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                raise
        else:
            raise ValueError(
                "ë¡œì»¬ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í˜•ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤:\n"
                "  1. í‘œì¤€ diffusers í˜•ì‹: text_encoder/, unet/, vae/ í´ë”\n"
                "  2. ë‹¨ì¼ íŒŒì¼ í˜•ì‹: sd_xl_base_1.0.safetensors, sdxl.vae.safetensors\n"
                "download_model_local.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
    else:
        print(f"ğŸŒ Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {MODEL_ID}")
        # Hugging Face ëª¨ë¸ ë¡œë“œ
        # from_pretrainedëŠ” torch_dtypeë§Œ ë°›ì§€ë§Œ, dtype ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•˜ê²Œ í‘œì‹œ
        pipe = AutoPipelineForText2Image.from_pretrained(
            MODEL_ID,
            torch_dtype=dtype,  # dtype ë³€ìˆ˜ ì‚¬ìš© (float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
            cache_dir=str(HF_CACHE_DIR),
            variant="fp16" if dtype in (torch.float16, torch.bfloat16) else None,
            use_safetensors=True,
        )
        
        # torch_dtypeìœ¼ë¡œ ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”
        # (from_pretrainedì˜ torch_dtype íŒŒë¼ë¯¸í„°ê°€ ì´ë¯¸ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì— ì ìš©ë¨)
        
        # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© (Hugging Face ëª¨ë¸)
        if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
            print("  ğŸ”¥ Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì¤‘...")
            pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                pipe.scheduler.config,
                use_karras=True,
            )
            print("  âœ… DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")

    # âœ… RTX 4060 8GB ìµœì í™” ì˜µì…˜
    
    # 1. xFormers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ (ê°€ì¥ ì¤‘ìš”!)
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸  xFormers í™œì„±í™” ì‹¤íŒ¨: {e}")
        # xFormers ì‹¤íŒ¨ ì‹œ attention slicing ì‚¬ìš©
        try:
            pipe.enable_attention_slicing(slice_size="auto")
            print("âœ… Attention Slicing í™œì„±í™” (xFormers ëŒ€ì²´)")
        except Exception:
            pass

    # 2. VAE Tiling (ê³ í•´ìƒë„/ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì•ˆì •ì„±) - í•„ìˆ˜!
    try:
        pipe.enable_vae_tiling()
        print("âœ… VAE Tiling í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
    except Exception as e:
        print(f"âš ï¸  VAE Tiling í™œì„±í™” ì‹¤íŒ¨: {e}")
    
    # 3. VAE Slicing (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)
    try:
        pipe.enable_vae_slicing()
        print("âœ… VAE Slicing í™œì„±í™” (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)")
    except Exception as e:
        print(f"âš ï¸  VAE Slicing í™œì„±í™” ì‹¤íŒ¨: {e}")

    # ë””ë°”ì´ìŠ¤ ì´ë™
    if DEVICE == "cuda" and torch.cuda.is_available():
        pipe = pipe.to("cuda")
        print(f"âœ… CUDA ë””ë°”ì´ìŠ¤ë¡œ ì´ë™: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        pipe = pipe.to("cpu")
        print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ (ëŠë¦¼)")

    _PIPELINE = pipe
    _PIPELINE_LOADED = True # íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ
    print("âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
    return _PIPELINE

def get_refiner_pipeline():
    """
    SDXL Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    í•„ìš”í•  ë•Œë§Œ ë¡œë“œí•˜ê³ , CPU offload ì‚¬ìš©
    """
    global _REFINER_PIPELINE, _REFINER_LOADED
    
    if _REFINER_PIPELINE is not None and _REFINER_LOADED:
        return _REFINER_PIPELINE
    
    if not USE_REFINER:
        return None
    
    print("ğŸ”„ Refiner íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘...")
    dtype = _torch_dtype()
    
    from pathlib import Path
    model_path = Path(MODEL_ID)
    is_local = model_path.exists() and (model_path / "model_index.json").exists()
    
    # Refiner ëª¨ë¸ ê²½ë¡œ í™•ì¸
    refiner_model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
    if is_local:
        # ë¡œì»¬ refiner íŒŒì¼ í™•ì¸
        refiner_file = model_path / "sd_xl_refiner_1.0.safetensors"
        if refiner_file.exists():
            print("  ğŸ“ ë¡œì»¬ Refiner íŒŒì¼ ê°ì§€ (ë‹¨ì¼ íŒŒì¼ í˜•ì‹ì€ ì•„ì§ ë¯¸ì§€ì›)")
            # ë¡œì»¬ refinerëŠ” ë‹¨ì¼ íŒŒì¼ í˜•ì‹ì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ í•„ìš”
            # ì¼ë‹¨ Hugging Face refiner ì‚¬ìš© (ë¡œì»¬ refinerëŠ” ë³µì¡í•¨)
            refiner_model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
        else:
            print("  âš ï¸  ë¡œì»¬ Refiner íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Hugging Face ëª¨ë¸ ì‚¬ìš©")
    
    # Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ (Img2Img íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
    try:
        refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
            refiner_model_id,
            torch_dtype=dtype,
            variant="fp16" if dtype in (torch.float16, torch.bfloat16) else None,
            use_safetensors=True,
            cache_dir=str(HF_CACHE_DIR),
        )
        
        # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
        if SCHEDULER_TYPE == "dpm++" and USE_KARRAS:
            print("  ğŸ”¥ Refinerì— Karras ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì¤‘...")
            refiner.scheduler = DPMSolverMultistepScheduler.from_config(
                refiner.scheduler.config,
                use_karras=True,
            )
            print("  âœ… Refiner DPM++ Multistep Scheduler (Karras) ì ìš© ì™„ë£Œ")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜
        try:
            refiner.enable_xformers_memory_efficient_attention()
            print("âœ… Refiner: xFormers ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”")
        except Exception:
            refiner.enable_attention_slicing(slice_size="auto")
            print("âœ… Refiner: Attention Slicing í™œì„±í™”")
        
        refiner.enable_vae_tiling()
        refiner.enable_vae_slicing()
        
        # CPU offloadë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ (8GB VRAM ìµœì í™”)
        refiner.enable_model_cpu_offload()
        print("âœ… Refiner: CPU Offload í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
        
        _REFINER_PIPELINE = refiner
        _REFINER_LOADED = True
        print("âœ… Refiner íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
        return _REFINER_PIPELINE
        
    except Exception as e:
        print(f"âŒ Refiner íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("  âš ï¸  Refiner ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        return None
